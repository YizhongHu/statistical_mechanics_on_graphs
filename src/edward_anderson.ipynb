{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize, LinearConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1\n",
    "B = 1\n",
    "\n",
    "def Hamiltonian(*state):\n",
    "    x0, x1, x2, y01, y02 = 2 * np.array(state) - 1\n",
    "    return beta / 2 * (x0 * x1 * y01 + x0 * x2 * y02) + B * x0\n",
    "\n",
    "\n",
    "def target_function(x):\n",
    "    mu = np.exp(x)\n",
    "    exp = np.sum(np.fromfunction(Hamiltonian, (2, 2, 2, 2, 2)) * mu)\n",
    "    rel_entr = np.sum(\n",
    "        np.log(mu) * mu\n",
    "    )  # Since eta is uniform, the KL-div is just the entropy\n",
    "\n",
    "    mu_marg = np.sum(mu, axis=(0, 1, 2, 4))\n",
    "    rel_marg_entr = np.sum(np.log(mu_marg) * mu_marg)\n",
    "\n",
    "    return exp - rel_entr + rel_marg_entr\n",
    "\n",
    "# This function is not used. It is not accurate at this point in time\n",
    "def grad_target_function(x):\n",
    "    mu = np.exp(x)\n",
    "    mu_marg = np.sum(mu, axis=(0, 1, 2, 4))\n",
    "    grad_exp = np.fromfunction(Hamiltonian, (2, 2, 2, 2, 2))\n",
    "    grad_rel_entr = np.log(mu) + 1\n",
    "    grad_rel_marg_entr = np.log(mu_marg) + 1\n",
    "    # repeat the values to all dimensions\n",
    "    grad_rel_marg_entr = np.tile(\n",
    "        np.tile(grad_rel_marg_entr[:, np.newaxis], (1, 2)), (2, 2, 2, 1, 1)\n",
    "    )\n",
    "    return (grad_exp - grad_rel_entr + grad_rel_marg_entr) * mu\n",
    "\n",
    "def constraint_function(x):\n",
    "    return np.sum(np.exp(x)) - 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Optimization for Constrained Optimization\n",
    "\n",
    "For the following optimization problem:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{maximize}\\quad& f(\\textbf{x}) \\\\\n",
    "\\text{subject to}\\quad& \\textbf{g}(\\textbf{x}) = \\textbf{0}\n",
    "\\end{aligned}$$\n",
    "\n",
    "We have the following iterative process to obtain an optimum:\n",
    "$$\\begin{aligned}\n",
    "\\textbf{x}^{(t+1)} &= \\text{argmax}_{\\text{x}} f(\\text{x}) + \\mathbf{\\lambda}^{(t)} \\cdot \\textbf{g}(\\textbf{x}) - \\frac12 (\\textbf{g}(\\textbf{x}))^2 \\\\\n",
    "\\mathbf{\\lambda}^{(t+1)} &= \\mathbf{\\lambda}^{(t)} - \\textbf{g}\\left(\\textbf{x}^{(t+1)}\\right) \\\\\n",
    "\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(x0, lam0, patience, eps, verbose=True):\n",
    "\n",
    "    x = x0\n",
    "    lam = lam0\n",
    "    \n",
    "    target = -np.inf\n",
    "    for it in range(patience):\n",
    "        if verbose:\n",
    "            print(f\"{it=}\")\n",
    "\n",
    "        # Redefine the function for scipy.optimize.minimize\n",
    "        # There is probably some fancy functional-programming way to incorporate lambda\n",
    "        # but you get the point.\n",
    "        def proximal_target_function(_x):\n",
    "            _x = _x.reshape((2, 2, 2, 2, 2))\n",
    "            _constraint = constraint_function(_x)\n",
    "            return -target_function(_x) - lam * _constraint + _constraint ** 2 / 2\n",
    "        \n",
    "        # Calculate proximal point\n",
    "        res = minimize(proximal_target_function, x0=x)\n",
    "        x = res.x.reshape((2, 2, 2, 2, 2))\n",
    "        \n",
    "        new_target = target_function(x)\n",
    "        constraint = constraint_function(x)\n",
    "        if verbose:\n",
    "            print(f\"{new_target=}\\t{constraint=}\")\n",
    "            \n",
    "        # Decide whether to terminate\n",
    "        if np.abs(target - new_target) < eps:\n",
    "            return x, target\n",
    "        target = new_target\n",
    "        \n",
    "        # Update lambda\n",
    "        lam = lam - constraint\n",
    "    return x, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it=0\n",
      "new_target=15.325634961650895\tconstraint=3.4465970351591215\n",
      "it=1\n",
      "new_target=3.446590107014522\tconstraint=-2.453636693600103e-06\n",
      "it=2\n",
      "new_target=3.446590107014522\tconstraint=-2.453636693600103e-06\n"
     ]
    }
   ],
   "source": [
    "x0 = np.zeros((2, 2, 2, 2, 2))\n",
    "\n",
    "x, _ = optimize(x0, lam0=0, patience=1000, eps=1e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "In our theory we have hypothesized that\n",
    "\n",
    "$$\\mathbb{P}_\\mu(S=s|Y_{01} = y_{01}) = \\frac1Z e^{\\mathcal{H}(s)}$$\n",
    "\n",
    "We will compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution Norm: 1.0000070533738148\n",
      "\n",
      "Checking for conditional distribution:\n",
      "Difference between mu and exponential distribution:\n",
      "[[[[[1.20245598e-07 1.01423153e-10]\n",
      "    [4.32244306e-10 2.00683444e-06]]\n",
      "\n",
      "   [[1.52212960e-07 3.88765382e-07]\n",
      "    [2.26945305e-10 2.74874765e-10]]]\n",
      "\n",
      "\n",
      "  [[[7.89363842e-11 1.15200089e-06]\n",
      "    [4.28817437e-10 1.41317703e-07]]\n",
      "\n",
      "   [[3.31741391e-08 2.16043087e-11]\n",
      "    [8.19818051e-08 1.67358305e-07]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[2.05974632e-07 1.04106014e-06]\n",
      "    [4.93942014e-07 5.31934754e-06]]\n",
      "\n",
      "   [[7.65289752e-07 5.40694162e-06]\n",
      "    [5.22204858e-06 7.09776612e-07]]]\n",
      "\n",
      "\n",
      "  [[[1.03778900e-07 2.82694247e-06]\n",
      "    [3.66998295e-05 4.34688571e-07]]\n",
      "\n",
      "   [[3.04157687e-06 8.11379460e-07]\n",
      "    [1.95319519e-07 2.19258515e-05]]]]]\n",
      "\n",
      "P(Y01=y01): [0.71504575 0.28496131]\n"
     ]
    }
   ],
   "source": [
    "mu = np.exp(x)\n",
    "print(f'Distribution Norm: {np.sum(mu)}\\n')\n",
    "print(f'Checking for conditional distribution:')\n",
    "# Conditional Distribution\n",
    "mu01 = np.sum(mu, (0, 1, 2, 4))\n",
    "mu_cond = mu / np.tile(np.tile(mu01[:, np.newaxis], (1, 2)), (2, 2, 2, 1, 1))\n",
    "# mu_cond0 = mu[:, :, :, 0, :] / mu01[0]\n",
    "# mu_cond1 = mu[:, :, :, 1, :] / mu01[1]\n",
    "\n",
    "# Exponential Distribution\n",
    "exponential_dist = np.exp(np.fromfunction(Hamiltonian, (2, 2, 2, 2, 2)))\n",
    "norm = np.sum(exponential_dist, (0, 1, 2, 4))\n",
    "norm = np.tile(np.tile(norm[:, np.newaxis], (1, 2)), (2, 2, 2, 1, 1))\n",
    "exponential_dist = exponential_dist / norm\n",
    "\n",
    "# exponential_dist0 = exponential_dist[:, :, :, 0, :]\n",
    "# exponential_dist0 = exponential_dist0 / np.sum(exponential_dist0)\n",
    "# exponential_dist1 = exponential_dist[:, :, :, 1, :]\n",
    "# exponential_dist1 = exponential_dist1 / np.sum(exponential_dist1)\n",
    "print(f'Difference between mu and exponential distribution:')\n",
    "print(np.abs(mu_cond - exponential_dist))\n",
    "\n",
    "print(f'\\nP(Y01=y01): {mu01}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing initial guess\n",
    "\n",
    "We will try the influence of the initial guess on x to the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Guess=[[[[[ 0.31565381  0.26116493]\n",
      "    [-0.52779404 -0.36706465]]\n",
      "\n",
      "   [[ 0.63012789  0.24220802]\n",
      "    [ 0.15937316  0.61637476]]]\n",
      "\n",
      "\n",
      "  [[[ 0.1435931  -0.19305956]\n",
      "    [-0.29644628 -0.77405785]]\n",
      "\n",
      "   [[-0.3878525  -0.72787086]\n",
      "    [-0.91334667  0.17384261]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[-0.968578   -0.17608896]\n",
      "    [-0.20617675 -0.40441563]]\n",
      "\n",
      "   [[-0.07337725 -0.44329344]\n",
      "    [ 0.95241968  0.8043892 ]]]\n",
      "\n",
      "\n",
      "  [[[-0.13593784 -0.52466993]\n",
      "    [ 0.69956671 -0.35113912]]\n",
      "\n",
      "   [[ 0.69803921  0.09689119]\n",
      "    [-0.61765896  0.33439266]]]]]\n",
      "target=array([[[[-17.55590668, -19.42241647],\n",
      "         [ -2.44656519,  -3.44652141]],\n",
      "\n",
      "        [[-22.37716224, -17.67363032],\n",
      "         [ -3.44652173,  -2.44656547]]],\n",
      "\n",
      "\n",
      "       [[[-22.45195296, -18.42333841],\n",
      "         [ -1.44656375,  -2.44656519]],\n",
      "\n",
      "        [[-18.84338958, -23.69857227],\n",
      "         [ -2.44656534,  -1.4465635 ]]]])\n"
     ]
    }
   ],
   "source": [
    "# x0 = np.ones((2, 2, 2, 2, 2))\n",
    "x0 = np.random.uniform(-1, 1, (2, 2, 2, 2, 2))\n",
    "print(f'Initial Guess={x0}')\n",
    "\n",
    "x, target = optimize(x0, lam0=0, patience=1000, eps=1e-6, verbose=False)\n",
    "print(f'{target=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution Norm: 0.9999938617730124\n",
      "\n",
      "Checking for conditional distribution:\n",
      "\n",
      "P(Y01=y01): [5.24609141e-08 9.99993809e-01]\n"
     ]
    }
   ],
   "source": [
    "mu = np.exp(x)\n",
    "print(f'Distribution Norm: {np.sum(mu)}\\n')\n",
    "print(f'Checking for conditional distribution:')\n",
    "# Conditional Distribution\n",
    "mu01 = np.sum(mu, (0, 1, 2, 4))\n",
    "print(f'\\nP(Y01=y01): {mu01}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all of them have different values of conditional probability, which proves our suspicion that it does not matter what the distribution\n",
    "on $Y_{01}$ is.\n",
    "\n",
    "Note 1: When $P(Y01=y01)$ is close to zero, the conditional probability value may not be as accurate. This will appear as 1e-1 and 1e-2 magnitude deviations\n",
    "Note 2: Some initial values may overflow. This seems to be normal. Try again with a different initial value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different values of $\\beta$ and $B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target=3.4466201992444496\n",
      "Distribution Norm: 1.0000062769301432\n",
      "\n",
      "\n",
      "P(Y01=y01): [0.65733047 0.34267581]\n",
      "[[0.05093462 0.0682694 ]\n",
      " [0.50442531 0.37637067]]\n",
      "covariance=-0.061066063086528816, sigma_x=0.9938516549676646, sigma_y=0.6480568629039271\n",
      "pearson=-0.09481242239440378\n"
     ]
    }
   ],
   "source": [
    "beta = 1\n",
    "B = 1\n",
    "\n",
    "x0 = np.random.uniform(-1, 1, (2, 2, 2, 2, 2))\n",
    "# x0 = np.ones((2, 2, 2, 2, 2))\n",
    "x, target = optimize(x0, lam0=0, patience=1000, eps=1e-10, verbose=False)\n",
    "print(f'{target=}')\n",
    "\n",
    "mu = np.exp(x)\n",
    "print(f'Distribution Norm: {np.sum(mu)}\\n')\n",
    "# print(f'Checking for conditional distribution:')\n",
    "# Conditional Distribution\n",
    "mu01 = np.sum(mu, (0, 1, 2, 4))\n",
    "# mu_cond = mu / np.tile(np.tile(mu01[:, np.newaxis], (1, 2)), (2, 2, 2, 1, 1))\n",
    "\n",
    "# # Exponential Distribution\n",
    "# exponential_dist = np.exp(np.fromfunction(Hamiltonian, (2, 2, 2, 2, 2)))\n",
    "# norm = np.sum(exponential_dist, (0, 1, 2, 4))\n",
    "# norm = np.tile(np.tile(norm[:, np.newaxis], (1, 2)), (2, 2, 2, 1, 1))\n",
    "# exponential_dist = exponential_dist / norm\n",
    "\n",
    "# print(f'Difference between mu and exponential distribution:')\n",
    "# print(np.abs(mu_cond - exponential_dist))\n",
    "\n",
    "print(f'\\nP(Y01=y01): {mu01}')\n",
    "\n",
    "a = 1\n",
    "b = 0\n",
    "axes = list(range(len(mu.shape)))\n",
    "axes.remove(a)\n",
    "axes.remove(b)\n",
    "dist = np.sum(mu, tuple(axes))\n",
    "dist = dist / np.sum(dist) # Normalizing may affect the results slightly, but we are estimating anyways\n",
    "print(dist)\n",
    "x_value = np.fromfunction(lambda x, y: (2 * x - 1), dist.shape)\n",
    "y_value = np.fromfunction(lambda x, y: (2 * y - 1), dist.shape)\n",
    "product = x_value * y_value\n",
    "covariance = np.sum(product * dist) - np.sum(x_value * dist) * np.sum(y_value * dist)\n",
    "\n",
    "dist_x = np.sum(dist, 0)\n",
    "variance_x = np.sum(dist_x) - (dist_x[1] - dist_x[0]) ** 2\n",
    "sigma_x = np.sqrt(variance_x)\n",
    "dist_y = np.sum(dist, 1)\n",
    "variance_y = np.sum(dist_y) - (dist_y[1] - dist_y[0]) ** 2\n",
    "sigma_y = np.sqrt(variance_y)\n",
    "\n",
    "print(f'{covariance=}, {sigma_x=}, {sigma_y=}')\n",
    "\n",
    "pearson = covariance / (sigma_x * sigma_y)\n",
    "print(f'{pearson=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.19201787e-01 7.31001211e-09]\n",
      " [8.80797800e-01 4.05690092e-07]]\n",
      "covariance=1.676813660145271e-07, sigma_x=0.0012853014181753298, sigma_y=0.6480516237847741\n",
      "pearson=0.00020131225490591998\n"
     ]
    }
   ],
   "source": [
    "a = 3\n",
    "b = 0\n",
    "axes = list(range(len(mu.shape)))\n",
    "axes.remove(a)\n",
    "axes.remove(b)\n",
    "dist = np.sum(mu, tuple(axes))\n",
    "dist = dist / np.sum(dist)\n",
    "print(dist)\n",
    "x_value = np.fromfunction(lambda x, y: (2 * x - 1), dist.shape)\n",
    "y_value = np.fromfunction(lambda x, y: (2 * y - 1), dist.shape)\n",
    "product = x_value * y_value\n",
    "covariance = np.sum(product * dist) - np.sum(x_value * dist) * np.sum(y_value * dist)\n",
    "\n",
    "dist_x = np.sum(dist, 0)\n",
    "variance_x = np.sum(dist_x) - (dist_x[1] - dist_x[0]) ** 2\n",
    "sigma_x = np.sqrt(variance_x)\n",
    "dist_y = np.sum(dist, 1)\n",
    "variance_y = np.sum(dist_y) - (dist_y[1] - dist_y[0]) ** 2\n",
    "sigma_y = np.sqrt(variance_y)\n",
    "\n",
    "print(f'{covariance=}, {sigma_x=}, {sigma_y=}')\n",
    "\n",
    "pearson = covariance / (sigma_x * sigma_y)\n",
    "print(f'{pearson=}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
