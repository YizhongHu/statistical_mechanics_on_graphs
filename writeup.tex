\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amsmath,amssymb,amsthm,mathabx}
\usepackage{hyperref}
\usepackage{fontspec}
\usepackage{setspace}
\usepackage[margin=1in]{geometry}
\usepackage[table,xcdraw]{xcolor}
\usepackage[textfont={rm,it}]{caption}
\usepackage{subcaption}
\usepackage{subfloat}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[american]{babel}
\usepackage{csquotes}


\usepackage[style=verbose]{biblatex}
\addbibresource{source/source.bib}

\allowdisplaybreaks

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}

\numberwithin{equation}{section}
\begin{document}
\title{Yizhong Hu Summer 2023 Results Report}
\author{Yizhong Hu}
\maketitle

\section{Jun 1 - Jun 5}

\subsection{Model Description}

We consider the Hamiltonian $\mathcal{H}: \mathcal{X}^{\kappa + 1} \times \mathcal{Y}^{\kappa} \mapsto \mathbb{R}$

\begin{equation*}
    \mathcal{H}(x_0, x_1, \ldots, x_\kappa, y_{01}, \ldots, y_{0\kappa} ) = \frac{\beta}{2}\sum_{i=1}^\kappa x_0x_iy_{0i} + Bx_0
\end{equation*}
where $x_0$ is spin state of the root node, $x_i$ is the spin state of the $i$-th leaf node, $y_{0i}$ is the interaction between
$0$ and $i$. All of them take values in $\mathcal{X} = \mathcal{Y} = \{-1, 1\}$.

When $\kappa=2$,
\begin{equation*}
    \mathcal{H}(x_0, x_1, x_2, y_{01}, y_{02} ) = \frac{\beta}{2}(x_0x_1y_{01} + x_0x_2y_{02}) + Bx_0
\end{equation*}

We define the state $s$ in state space $\mathbb{S} = \mathcal{X}^3 \times \mathcal{Y}^2$, and the random variable $S$ a random variable
on $\mathbb{S}$. Given $\eta$ the i.i.d Bernoulli(1/2) distribution on $\mathbb{S}$, we want to investigate the distribution $\mu(\cdot)$ that maximizes
\begin{equation*}
    \max_{\mu(\cdot)} \left\{\mathbb{E}_\mu[\mathcal{H}(S)] - \left[H(\mu \| \eta) + H(\mu_{01}\|\eta_{01} )\right]\right\}
\end{equation*}

\subsection{Analytical analysis}

First, we need to find the distribution $\mu: \mathcal{X}^3 \times \mathcal{Y}^2 \mapsto [0, 1]$. Since the input is discrete,
$\mu$ can be rewritten as a vector on $[0, 1]^{2^5}$. We will denote each component as $\mu(s)$, with $s\in \mathbb{S}$

The marginal distribution is therefore
\begin{equation*}
    \mu_{01}(y_{01}) = \sum_{x_1, x_2, x_3, y_{02} \in \{-1, 1\}} \mu(s)
\end{equation*}

Rewriting each term,
\begin{align*}
    \mathbb{E}_\mu[\mathcal{H}(S)] & = \sum_{s\in \mathbb{S}} \mu(s) \mathcal{H}(s)                                                 \\
    H(\mu \| \eta)                 & = \sum_{s\in \mathbb{S}} \mu(s) \log\frac{\mu(s)}{\eta(s)}                                     \\
    H(\mu_{01} \| \eta_{01})       & = \sum_{y_{01}\in \mathcal{Y}} \mu_{01}(y_{01}) \log\frac{\mu_{01}(y_{01})}{\eta_{01}(y_{01})}
\end{align*}
Note that since $\eta$ is uniform, the relative entropies can be written directly in terms of their entropies:
\begin{align*}
    H(\mu \| \eta)           & = H(\mu) - \log |\mathbb{S}| \\
    H(\mu_{01} \| \eta_{01}) & = H(\mu_{01}) - \log 2
\end{align*}
Rewriting the target,
\begin{equation*}
    \max_{\mu(\cdot)} \left[\sum_{s\in \mathbb{S}} \mu(s) \mathcal{H}(s) - \sum_{s\in \mathbb{S}} \mu(s) \log \mu(s) + \sum_{y_{01}\in \mathcal{Y}} \mu_{01}(y_{01}) \log\mu_{01}(y_{01})\right]
\end{equation*}

The corresponding Lagrange multiplier (constrained on $\mu$ being normalized) is
\begin{equation*}
    \mathcal{L} =\left[\sum_{s\in \mathbb{S}} \mu(s) \mathcal{H}(s) - \sum_{s\in \mathbb{S}} \mu(s) \log \mu(s) + \sum_{y_{01}\in \mathcal{Y}} \mu_{01}(y_{01}) \log\mu_{01}(y_{01})\right] + \lambda\left[\sum_{s\in \mathbb{S}} \mu(s) - 1\right]
\end{equation*}
taking the gradients gives
\begin{align}
    \frac{\partial \mathcal{L}}{\partial \mu(s)}  & = \mathcal{H}(s) - \log \mu(s) + \log \mu_{01}(y_{01}) + \lambda = 0 \\
    \frac{\partial \mathcal{L}}{\partial \lambda} & = \sum_{s\in \mathbb{S}} \mu(s) - 1 = 0
\end{align}
where $y_{01}$ in Eq.(1) represents the value of $y_{01}$ in $s$.

Eq.(1) gives the form of $\mu$ in terms of a conditional distribution, as seen below:
\begin{align*}
    \log \mu(s) - \log \mu_{01}(y_{01})  & = \mathcal{H}(s) + \lambda     \\
    \log \frac{\mu(s)}{\mu_{01}(y_{01})} & = \mathcal{H}(s) + \lambda     \\
    \frac{\mu(s)}{\mu_{01}(y_{01})}      & = e^\lambda e^{\mathcal{H}(s)}
\end{align*}
The left-hand side is a conditional distribution:
\begin{equation}
    \mathbb{P}_\mu (S = s | Y_{01} = y_{01}) = \frac1Z \exp\left[\frac{\beta}{2}(x_0x_1y_{01} + x_0x_2y_{02}) + Bx_0\right]
\end{equation}
for some $Z = e^{-\lambda} = $ normalizing constant.

Since $\frac{\partial \mathcal{L}}{\partial \mu(s)} = 0$ for all $\mu_{01}$ values, and all such optimum points form a connected curve,
we think that it is reasonable to hypothesize that any choice of $\mu_{01}$ satisfies optimality.
Since $y_{01}$ can only take two values, the distribution can be characterized by a single real value $\alpha$:
\begin{equation*}
    \mathbb{P}_\mu(Y_{01} = y_{01}) = \frac{e^{\alpha y_{01}}}{e^\alpha + e^{-\alpha}},
\end{equation*}
and the entire distribution becomes
\begin{equation*}
    \mu(s) = \frac1Z \exp\left[\frac{\beta}{2}(x_0x_1y_{01} + x_0x_2y_{02}) + Bx_0 + \alpha y_{01}\right]
\end{equation*}

\subsection{Numerical Analysis}

On the other hand, this problem can be solved numerically, formulated as a constrained optimization:

\begin{align*}
    \text{maximize}\quad   & \sum_{s\in \mathbb{S}} \mu(s) \mathcal{H}(s) - \sum_{s\in \mathbb{S}} \mu(s) \log \mu(s) + \sum_{y_{01}\in \mathcal{Y}} \mu_{01}(y_{01}) \log\mu_{01}(y_{01}) \\
    \text{subject to}\quad & \sum_{s\in\mathbb{S}} \mu(s) = 1
\end{align*}

Since entropy calculations really don't like getting negative values, we will represent $\mu$ as an exponential:
\begin{equation*}
    \mu(s) = e^{x(s)}
\end{equation*}
Note that since $\exp: \mathbb{R} \mapsto (0, \infty)$ is bijective, we do not lose any generality.

A proximal point optimization method will be used for the optimization. For the following optimization problem

$$\begin{aligned}
        \text{maximize}\quad   & f(\textbf{x})                        \\
        \text{subject to}\quad & \textbf{g}(\textbf{x}) = \textbf{0},
    \end{aligned}$$
we have the following iterative process to obtain an optimum:
$$\begin{aligned}
        \textbf{x}^{(t+1)}       & = \text{argmax}_{\text{x}} f(\text{x}) + \mathbf{\lambda}^{(t)} \cdot \textbf{g}(\textbf{x}) - \frac12 (\textbf{g}(\textbf{x}))^2 \\
        \mathbf{\lambda}^{(t+1)} & = \mathbf{\lambda}^{(t)} - \textbf{g}\left(\textbf{x}^{(t+1)}\right)                                                              \\
    \end{aligned}$$
The minimization on $\textbf{x}$ is done with the SciPy minimize. The code is provided in the notebook attached.

Results from numerical analysis confirm the results from analytical analysis. Aside from inaccuracies introduced by low value of $\mu_{01}(y_{01})$,
the conditional distribution given $Y_{01}$ matches the exponential distribution $\exp[\mathcal{H}(s)]$ to around $10^{-6}$ accuracy. Additionally,
any distribution on $Y_{01}$ can be optimal, which conforms with the analytical understanding.

\subsection{Correlation}

To see the correlation, we will try to calculate the Pearson coefficient for the pairs of random variables. We will test these on different choices of $\beta$ and $B$. To make sure that the results are accurate, we reduced the accuracy requirement to below $10^{-10}$.

\begin{itemize}
    \item For $Y^*_{01}$, and $X^*_{0}$, we can see that the Pearson coefficient is very close to $0$ except for when $B$ is much larger than $1$ and $Y^*_{01}$
          is skewed to one of the results, which could be just an accuracy issue. We hence hypothesize that $Y^*_{01}$, and $X^*_{0}$ are independent.
          To make it more certain, we can try numerically calculating the distribution in exponential form to confirm.
\end{itemize}




It is unclear what correlation is like in this situation. Given how the distribution is structured, we can separate the exponent additively to obtain
mutually independent pieces. For example, if we fix $x_0$, we know that $X_1, Y_{01}$ and $X_2, Y_{02}$ are independent of each other:
\begin{equation*}
    \mu(s) = \frac1Z \exp(Bx_0) \exp\left[\frac{\beta}{2}(x_0x_1y_{01}) + \alpha y_{01}\right] \exp\left[ \frac\beta2 x_0x_2y_{02}\right]
\end{equation*}
I have tried to calculate the covariance between different random variables from the numerical results, but they differ by the choice of $\mu_{01}$,
and the marginal distributions don't seem like there is a definitive answer for if they correlate or are independent. I may need more guidance on this
issue.

\newpage

\section{Jun 6 - Jun 12}

To simplify, we still consider the Ising model.

\subsection{Numerical Analysis}

\subsubsection{Method}

We vectorize the state, representing the probability as $\mu_{x_0k}$, where $x_0$ is the spin state for the root node, and $k$ leaves
are in the spin-up state. The Hamiltonian can then be rewritten
\begin{equation*}
    \mathcal{H}(x_0, k) = \frac{\beta}{2}x_0(2k-\kappa) + Bx_0.
\end{equation*}
The edge distribution is therefore, for some distribution $p$,
\begin{equation*}
    \pi_p(x_0, x_v)=\frac1\kappa\sum_{k=1}^\kappa p_{x_0k}\left[\mathbf{1}_{\{x_v=1\}}k + \mathbf{1}_{\{x_v=-1\}}(\kappa-k)\right].
\end{equation*}
Note that if we represent $\mu\in\mathbb{R}^{2\times {(\kappa+1)}}$, this can be represented as a matrix multiplication.

The underlying distribution needs to account for multiplicity as well:
\begin{equation*}
    \eta_{x_0k} = {\kappa \choose k} 2^{-\kappa}.
\end{equation*}
Hence, the optimization problem becomes
\begin{align*}
    \text{maximize}\quad   & \sum_{x_0\in\mathcal{X}}\sum_{k=0}^\kappa \mu_{x_0k}\mathcal{H}(x_0, k) - \sum_{x_0\in\mathcal{X}}\sum_{k=0}^\kappa \mu_{x_0k}\log\frac{\mu_{x_0k}}{\eta_{x_0k}} + \frac{\kappa}{2} \sum_{x_0\in\mathcal{X}}\sum_{x_v\in\mathcal{X}} \pi_\mu(x_0, x_v)\log\frac{\pi_\mu(x_0, x_v)}{\pi_\eta(x_0, x_v)} \\
    \text{subject to}\quad & \sum_{x_0\in\mathcal{X}}\sum_{k=0}^\kappa \mu_{x_0k} = 1                                                                                                                                                                                                                                               \\
                           & \pi_\mu(1, -1) = \pi_\mu(-1, 1).
\end{align*}
Like last time, to guarantee positivity, we instead optimize for some $\xi$ such that $\mu_{x_0k} = \exp(\xi_{x_0k})$. And again,
we optimize with a proximal point method. Proximal points are calculated with L-BFGS-B from SciPy. The tolerances of the optimization target, $\mu^*$, and constraints are $10^{-6}$.

\subsubsection{Objects of interest}

\begin{itemize}
    \item First consider $X_0$ and $X_1$, where $X_1$ is a leaf chosen uniformly. The joint distribution is
          \begin{equation*}
              \mathbb{P}_{\mu}(X_0=x_0, X_1=x_1) = \pi_\mu(x_0, x_1).
          \end{equation*}
          Since $\pi_\mu(x_0, x_1)$ is symmetric, they are identically distributed.
          From numerical analysis, we find that only when $\beta$ is $0$ are $X_0$ and $X_1$ independent.
          When $\beta > 0$, there is a preference of $X_0$ and $X_1$ being the same spins, and vice versa.
          For $B=0$, As $|\beta| \rightarrow \infty$, we see that $X_0$ and $X_1$ goes to Bernoulli(1/2).
    \item Then we consider $X_1$ and $X_2$, where they are distinct leaves chosen uniformly:
          \begin{equation*}
              \mathbb{P}_{\mu}(X_1=x_1, X_2=x_2) = \frac{1}{{\kappa \choose k}} \sum_{x_0\in\mathcal{X}} \mu_{x_0k} ,
          \end{equation*}
          where $k$ is the number of spin-up states in $\{x_1, x_2\}$
          % \begin{equation*}
          %     \nu(x_1, x_2; \kappa, k) = \mathbb{P}_{\mu}(X_1=x_1, X_2=x_2 | k) = \frac{1}{{\kappa \choose 2}}\begin{cases}
          %         {k \choose 2} & x_1=x_2=1 \\
          %         \frac12{k \choose 1}{\kappa - k \choose 1} & x_1\neq x_2 \\
          %         {\kappa - k \choose 2} & x_1=x_2=-1 
          %     \end{cases}
          % \end{equation*}
          Results from the simulation conclude that $X_1$ and $X_2$ are only independent when $\beta=0$. The higher the $\beta$
          is, we approach $X_1=X_2$.
    \item For $X_1$ and $X_2$ conditioned under $X_0$, we have
          \begin{equation*}
              \mathbb{P}_{\mu}(X_1=x_1, X_2=x_2 | X_0 = x_0) = \frac{1}{{\kappa \choose k}} \mu_{x_0k}.
          \end{equation*}
          Again, $X_1$ and $X_2$ are only independent when $\beta=0$. When $\beta>0$, they tend to equal $X_0$, and when $\beta<0$,
          they tend to equal $-X_0$, consistent with the first conclusion.
\end{itemize}

This points towards a representation of $\mu^*$ similar to the form
\begin{equation*}
    \mu^*(x_0, x_1, x_2) = \frac1Z\exp(\mathcal{H}(x_0, x_1, x_2)).
\end{equation*}

\newpage

\subsection{Analytical solution}

The corresponding Lagrange multiplier is
\begin{multline*}
    \mathcal{L} = \sum_{x_0\in\mathcal{X}}\sum_{k=0}^\kappa \mu_{x_0k}\mathcal{H}(x_0, k)
    - \sum_{x_0\in\mathcal{X}}\sum_{k=0}^\kappa \mu_{x_0k}\log\frac{\mu_{x_0k}}{\eta_{x_0k}}
    + \frac{\kappa}{2} \sum_{x_0\in\mathcal{X}}\sum_{x_v\in\mathcal{X}} \pi_\mu(x_0, x_v)\log\frac{\pi_\mu(x_0, x_v)}{\pi_\eta(x_0, x_v)} \\
    + \lambda_1 \left(\sum_{x_0\in\mathcal{X}}\sum_{k=0}^\kappa \mu_{x_0k} - 1\right)
    + \lambda_2 \left[\pi_\mu(-1, 1) - \pi_\mu(1, -1)\right].
\end{multline*}
To take partial derivatives, first note that
\begin{align*}
    \frac{\partial \pi_\mu(x_0, 1)}{\partial \mu_{x_0k}}  & = \frac{k}{\kappa}           \\
    \frac{\partial \pi_\mu(x_0, -1)}{\partial \mu_{x_0k}} & = \frac{\kappa - k}{\kappa}.
\end{align*}
Then, we have
\begin{multline*}
    \frac{\partial \mathcal{L}}{\partial \mu_{x_0k}} = \mathcal{H}(x_0, k)
    - \log\frac{\mu_{x_0k}}{\eta_{x_0k}} - 1
    + \frac{\kappa}{2}\left\{
    \left[\log\frac{\pi_\mu(x_0, 1)}{\pi_\eta(x_0, 1)}+1\right]\frac{k}{\kappa} +
    \left[\log\frac{\pi_\mu(x_0, -1)}{\pi_\eta(x_0, 1)}+1\right]\frac{\kappa - k}{\kappa}
    \right\} \\
    + \lambda_1
    + \lambda_2 \left(\mathbf{1}_{\{x_0=1\}}\frac{\kappa - k}{\kappa} -
    \mathbf{1}_{\{x_0=-1\}}\frac{k}{\kappa}\right) = 0,
\end{multline*}
which can be simplified to
\begin{multline*}
    \log\frac{\mu_{x_0k}}{\eta_{x_0k}} = \mathcal{H}(x_0, k)
    + \frac{1}{2}\left[
        k\log\frac{\pi_\mu(x_0, 1)}{\pi_\eta(x_0, 1)} +
        (\kappa - k)\log\frac{\pi_\mu(x_0, -1)}{\pi_\eta(x_0, 1)}
        \right] + \frac{\kappa}{2} - 1\\
    + \lambda_1
    + \lambda_2 \left(\mathbf{1}_{\{x_0=1\}}\frac{\kappa - k}{\kappa} -
    \mathbf{1}_{\{x_0=-1\}}\frac{k}{\kappa}\right)
\end{multline*}
Hence, we can conclude
\begin{equation*}
    \mu_{x_0k} = \frac1Z \eta_{x_0k} \left[\frac{\pi_\mu(x_0, 1)}{\pi_\eta(x_0, 1)}\right]^{k/2} \left[\frac{\pi_\mu(x_0, -1)}{\pi_\eta(x_0, 1)}\right]^{(\kappa - k)/2} \exp\left[\mathcal{H}(x_0, k) + \lambda_2 \left(\mathbf{1}_{\{x_0=1\}}\frac{\kappa - k}{\kappa} -
        \mathbf{1}_{\{x_0=-1\}}\frac{k}{\kappa}\right)\right]
\end{equation*}
where
\begin{equation*}
    Z = \sum_{x_0\in\mathcal{X}}\sum_{k=0}^\kappa \eta_{x_0k} \eta_{x_0k} \left[\frac{\pi_\mu(x_0, 1)}{\pi_\eta(x_0, 1)}\right]^{k/2} \left[\frac{\pi_\mu(x_0, -1)}{\pi_\eta(x_0, 1)}\right]^{(\kappa - k)/2} \exp\left[\mathcal{H}(x_0, k) + \lambda_2 \left(\mathbf{1}_{\{x_0=1\}}\frac{\kappa - k}{\kappa} -
        \mathbf{1}_{\{x_0=-1\}}\frac{k}{\kappa}\right)\right]
\end{equation*}
This is, however, not a closed-form solution, as $\pi_\mu$ is still a linear combination of elements in $\mu$

\pagebreak

\section{Jun 14 - Jun 20}

\subsection{Objects of interest}

A proximal point method was used, but it may not be as reliable as some SciPy implementations. SLSQP is used instead
to find the optimum. This guarantees that all the guesses satisfy constraints. The tolerance for the objective function is $10^{-10}$.
This seems to result in $\mu^*$ that is accurate to $10^{-5}$.

\subsubsection{Investigating $\mathbb{P}_{\mu^*}(X_0, X_v)$}

As mentioned, we know that $\mathbb{P}_{\mu^*}(X_0=x_0, X_v=x_v) = \pi_{\mu^*}(x_0, x_v)$
For independence, we need to show that
\begin{equation*}
    \mathbb{P}_{\mu^*}(X_0=x_0, X_v=x_v) =  \mathbb{P}_{\mu^*}(X_0=x_0) \mathbb{P}_{\mu^*}(X_v=x_v)
\end{equation*}

When $\beta=0$, $X_0$ and $X_v$ are independent (Tab.\ref{Tab. beta0B0}):
\begin{table}[h]
    \centering
    \begin{tabular}{c|rr}
        $\mathbb{P}_{\mu^*}(X_0, X_v)$ & \multicolumn{1}{c}{$X_v=-1$} & \multicolumn{1}{c}{$X_v=1$} \\ \hline
        $X_0=-1$                       & 0.25                         & 0.25                        \\
        $X_0=1$                        & 0.25                         & 0.25
    \end{tabular}
    \quad\quad
    \begin{tabular}{c|rr}
        $\mathbb{P}_{\mu^*}(X_0)\mathbb{P}_{\mu^*}(X_v)$ & \multicolumn{1}{c}{$X_v=-1$} & \multicolumn{1}{c}{$X_v=1$} \\ \hline
        $X_0=-1$                                         & 0.25                         & 0.25                        \\
        $X_0=1$                                          & 0.25                         & 0.25
    \end{tabular}
    \caption{When $\beta=0$ and $B=0$, up to $10^{-5}$ accuracy, the product of the marginals is equal to the join distribution}
    \label{Tab. beta0B0}
\end{table}

\noindent The higher the value for $\beta$, the more correlated $X_0$ and $X_v$ are. As $\beta\rightarrow\infty$, the correlation becomes stronger,
approaching $X_0=X_v$ (Tab.\ref{Tab. beta-higher-B0}).

\begin{table}[h]
    \raggedright
    $\beta=1$\\
    \centering
    \begin{tabular}{c|rr}
        $\mathbb{P}_{\mu^*}(X_0, X_v)$ & \multicolumn{1}{c}{$X_v=-1$} & \multicolumn{1}{c}{$X_v=1$} \\ \hline
        $X_0=-1$                       & 0.44                         & 0.06                        \\
        $X_0=1$                        & 0.06                         & 0.44
    \end{tabular}
    \quad\quad
    \begin{tabular}{c|rr}
        $\mathbb{P}_{\mu^*}(X_0)\mathbb{P}_{\mu^*}(X_v)$ & \multicolumn{1}{c}{$X_v=-1$} & \multicolumn{1}{c}{$X_v=1$} \\ \hline
        $X_0=-1$                                         & 0.25                         & 0.25                        \\
        $X_0=1$                                          & 0.25                         & 0.25
    \end{tabular} \\
    \raggedright
    $\beta=3$\\
    \centering
    \begin{tabular}{c|rr}
        $\mathbb{P}_{\mu^*}(X_0, X_v)$ & \multicolumn{1}{c}{$X_v=-1$} & \multicolumn{1}{c}{$X_v=1$} \\ \hline
        $X_0=-1$                       & 0.499                        & 0.001                       \\
        $X_0=1$                        & 0.001                        & 0.499
    \end{tabular}
    \quad\quad
    \begin{tabular}{c|rr}
        $\mathbb{P}_{\mu^*}(X_0)\mathbb{P}_{\mu^*}(X_v)$ & \multicolumn{1}{c}{$X_v=-1$} & \multicolumn{1}{c}{$X_v=1$} \\ \hline
        $X_0=-1$                                         & 0.25                         & 0.25                        \\
        $X_0=1$                                          & 0.25                         & 0.25
    \end{tabular} \\
    \raggedright
    $\beta=10$\\
    \centering
    \begin{tabular}{c|rr}
        $\mathbb{P}_{\mu^*}(X_0, X_v)$ & \multicolumn{1}{c}{$X_v=-1$} & \multicolumn{1}{c}{$X_v=1$} \\ \hline
        $X_0=-1$                       & 0.50                         & 0.00                        \\
        $X_0=1$                        & 0.00                         & 0.50
    \end{tabular}
    \quad\quad
    \begin{tabular}{c|rr}
        $\mathbb{P}_{\mu^*}(X_0)\mathbb{P}_{\mu^*}(X_v)$ & \multicolumn{1}{c}{$X_v=-1$} & \multicolumn{1}{c}{$X_v=1$} \\ \hline
        $X_0=-1$                                         & 0.25                         & 0.25                        \\
        $X_0=1$                                          & 0.25                         & 0.25
    \end{tabular}
    \caption{$\beta$ takes values 1, 3, and 10 and $B=0$. Notice that
        as $\beta$ increases, the correlation between $X_0$ and $X_v$ becomes stronger. They are no longer independent. They are positively correlated, i.e. $0$ and $v$ prefer being in the same
        state. It is hence reasonable to postulate that as $\beta\rightarrow\infty$, we will have $X_0=X_v$}
    \label{Tab. beta-higher-B0}
\end{table}

\noindent If $\beta < 0$, $X_0$ and $X_v$ are negatively correlated, and again, as $|\beta|\rightarrow \infty$, the correlation becomes stronger,
approaching $X_0=-X_v$ (Tab.\ref{Tab. beta-neg-B0}).

\begin{table}[h]
    \raggedright
    $\beta=-1$\\
    \centering
    \begin{tabular}{c|rr}
        $\mathbb{P}_{\mu^*}(X_0, X_v)$ & \multicolumn{1}{c}{$X_v=-1$} & \multicolumn{1}{c}{$X_v=1$} \\ \hline
        $X_0=-1$                       & 0.060                        & 0.440                       \\
        $X_0=1$                        & 0.440                        & 0.0596
    \end{tabular}
    \quad\quad
    \begin{tabular}{c|rr}
        $\mathbb{P}_{\mu^*}(X_0)\mathbb{P}_{\mu^*}(X_v)$ & \multicolumn{1}{c}{$X_v=-1$} & \multicolumn{1}{c}{$X_v=1$} \\ \hline
        $X_0=-1$                                         & 0.250                        & 0.250                       \\
        $X_0=1$                                          & 0.250                        & 0.250
    \end{tabular} \\
    \raggedright
    $\beta=-3$\\
    \centering
    \begin{tabular}{c|rr}
        $\mathbb{P}_{\mu^*}(X_0, X_v)$ & \multicolumn{1}{c}{$X_v=-1$} & \multicolumn{1}{c}{$X_v=1$} \\ \hline
        $X_0=-1$                       & 0.001                        & 0.499                       \\
        $X_0=1$                        & 0.499                        & 0.001
    \end{tabular}
    \quad\quad
    \begin{tabular}{c|rr}
        $\mathbb{P}_{\mu^*}(X_0)\mathbb{P}_{\mu^*}(X_v)$ & \multicolumn{1}{c}{$X_v=-1$} & \multicolumn{1}{c}{$X_v=1$} \\ \hline
        $X_0=-1$                                         & 0.250                        & 0.250                       \\
        $X_0=1$                                          & 0.250                        & 0.250
    \end{tabular} \\
    \raggedright
    $\beta=-10$\\
    \centering
    \begin{tabular}{c|rr}
        $\mathbb{P}_{\mu^*}(X_0, X_v)$ & \multicolumn{1}{c}{$X_v=-1$} & \multicolumn{1}{c}{$X_v=1$} \\ \hline
        $X_0=-1$                       & 0.000                        & 0.500                       \\
        $X_0=1$                        & 0.500                        & 0.000
    \end{tabular}
    \quad\quad
    \begin{tabular}{c|rr}
        $\mathbb{P}_{\mu^*}(X_0)\mathbb{P}_{\mu^*}(X_v)$ & \multicolumn{1}{c}{$X_v=-1$} & \multicolumn{1}{c}{$X_v=1$} \\ \hline
        $X_0=-1$                                         & 0.250                        & 0.250                       \\
        $X_0=1$                                          & 0.250                        & 0.250
    \end{tabular}
    \caption{$\beta$ takes values -1, -3, and -10 and $B=0$. As $\beta$ increases, the correlation between $X_0$ and $X_v$ becomes stronger.  They are negatively correlated, i.e. $0$ and $v$ prefer being in the different states. It is hence reasonable to postulate that as $\beta\rightarrow-\infty$, we will have $X_0=-X_v$}
    \label{Tab. beta-neg-B0}
\end{table}

\newpage

We can take more samples of $\beta$ to see a clearer trend.
\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{img/ising_x0_xv.png}
    \caption{
        We see the correlations for different $\beta$ and $B$ values and $B=0$.
        If and only if $\frac{\pi_{\mu}(x_0, x_v)}{P(x_0)P(x_v)}=1$,
        i.e. $\pi_{\mu}(x_0, x_v) = P(x_0)P(x_v)$ for all $x_0$ and all $x_v$,
        does it mean that $X_0$ and $X_v$ are independent. This only happens at $\beta=0$. The further the values deviate from $1$,
        the more dependence there is. Additionally, We see that as $\beta$ gets
        larger, the $(+, +)$ and $(-, -)$ states are more preferred, and the opposite is true when $\beta$ gets more negative.}
    \label{Fig. x0-xv-correlation}
\end{figure}

$X_0$ and $X_v$ are identically distributed by admissibility. For $B=0$, both $X_0$ and $X_v$ are Bernoulli(1/2). It changes when $B\neq 0$. More
specifically, when $B\rightarrow\infty$, $X_0$ approaches deterministically spin-up. When $B\rightarrow\infty$, $X_0$ approaches deterministically spin-down.
Additionally, the larger $\beta$ is, the faster the switch from spin-down to spin-up is.

\newpage

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{img/ising_x0_B.png}
    \caption{$X_0$ prefers $1$ when $B > 0$ and prefers $-1$ when $B < 0$. It is Bernoulli(1/2) when $B=0$. The rate at which $\mathbb{P}(X_0=1)$ increases with $B$ is dependent on $\beta$. The higher the value of $\beta$, the faster it increases.}
    \label{Fig.ising-x0-B}
\end{figure}

For correlation between $X_0$ and $X_v$ when $B\neq 0$. We see that as $|B|$
increases, the negative correlation for $\beta<0$ still goes to $X_0=-X_v$.
However, as $\beta\rightarrow\infty$, we no longer approach $X_0=X_v$. The larger
$|B|$ is, the weaker the correlation between $X_0$ and $X_v$ are at
$\beta\rightarrow\infty$.

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{img/ising_x0_xv_pearson.png}
    \caption{Correlation strength at $\beta\rightarrow\infty$ for $B\neq 0$ reduces
        as $|B|$ increases. The Pearson coefficient $R=\text{Cov}(X_0, X_v)/\sqrt{\text{Var}(X_0)\text{Var}(X_v)}$ is used to capture the strength of the correlation. Since $X_0$ and $X_v$ are Bernoulli, $R=0$ iff. $X_0, X_v$ independent. We see that $X_0, X_v$ independent iff. $\beta=0$.}
    \label{Fig. ising_x0_xv_B}
\end{figure}

We think that this is due to $B$ strongly biasing to one of the spin-up or spin-down
states, especially at high $\beta$ values. Since $X_0$ and $X_v$ are identically distributed, this results in the highest probability concentrated in one of the states. Pearson coefficient measures the strength of linear correlation, which, when
there is only one state, becomes less accurate.

Since our results are numerical,
and Pearson coefficient depends on the accuracy of small probability, we think that
the trend need to be investigated more to get more conclusive results. The above theory
also does not explain the asymmetry between $\beta\rightarrow\infty$ and $\beta\rightarrow-\infty$.

\subsubsection{Investigating $\mathbb{P}_{\mu^*}(X_1, X_2)$}

For $B=0$, a similar pattern can be observed with $X_1$ and $X_2$. However, this time, as
$|\beta|\rightarrow\infty$, it approaches $X_1=X_2$ no matter if $\beta$ is positive or
negative.

\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{./img/ising_x1_x2.png}
    \caption{
        When $B=0$, Similar to $X_0$ and $X_v$, when $\beta=0$, $X_1$ and $X_2$ are independent.
        When $\beta\neq 0$, they become correlated. Their correlation becomes stronger
        as $|\beta|\rightarrow\infty$, and it approaches $X_1=X_2$. Notice that $X_1$ and $X_2$
        are positively correlated in both $\beta > 0$ and $\beta < 0$.}
    \label{Fig.ising-x1-x2}
\end{figure}

For $B\neq 0$, $X_1$ and $X_2$ are still positively-correlated. When $\beta\rightarrow-\infty$, they still approach $X_1=X_2$. However, when
$|B|$ increases, the correlation of $X_1$ and $X_2$ at $\beta\rightarrow\infty$
becomes weaker, similar to the case between $X_0$ and $X_v$.
\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{img/ising_x1_x2_pearson.png}
    \caption{We see that for $B\neq 0$, the correlation is always a positive correlation, with independence happening iff $\beta=0$.}
    \label{Fig.ising-x1-x2-B}
\end{figure}

\subsubsection{Investigating $\mathbb{P}_{\mu^*}(X_1, X_2|X_0)$}

Given $X_0$, $X_1$ and $X_2$ are always conditionally independent

\begin{figure}[h]
    \centering
    \includegraphics[width=7cm]{img/ising_x1_x2_pearson_x0=-1.png}
    \quad
    \includegraphics[width=7cm]{img/ising_x1_x2_pearson_x0=1.png}
    \caption{The conditional distributions $X_1|X_0$ and $X_2|X_0$ are always independent}
    \label{Fig.ising-x1-x2-given-x0-B}
\end{figure}

given  suggests a numerical solution in the form
\begin{equation*}
    \mu(x_0,x_1,x_2) = \mu_v(x_1|x_0)\mu_v(x_2|x_0)\mu_0(x_0)
\end{equation*}

\subsection{Partial Analytical solution}

If we assume that $X_0$ and all $X_v$ are identically distributed (given by the admissibility constraint), and that $X_v$ are mutually conditionally independent from each other given $X_0$, we can see that
a distribution can be completely described by two parameters. (This is not
really what the question is asking, but the best I can come up
with.)

First, the probability distribution for $X_0$:
\begin{equation*}
    \mu_0(x):=\mathbb{P}_\mu(X_0=x)
\end{equation*}
This only needs one parameter, as $\mu_0(-)=1-\mu_0(+)$.

Secondly, we need the conditional distribution $X_v|X_0$:
\begin{equation*}
    \nu_\mu(x_0, x) := \mathbb{P}_\mu(X_v=x | X_0=x_0) = \frac{\pi_\mu(x_0, x_v)}{\mu_0(x_0)}
\end{equation*}
This describes $\pi_\mu$. Say $\nu_\mu^- := \nu_\mu(+, -)$, by admissibility,
\begin{align*}
    \pi_\mu(+, +) & = (1-\nu_\mu^-)\mu_0(+)   \\
    \pi_\mu(+, -) & = \nu_\mu^-\mu_0(+)       \\
    \pi_\mu(-, +) & = \nu_\mu^-\mu_0(+)       \\
    \pi_\mu(-, -) & = 1-(1+\nu_\mu^-)\mu_0(+)
\end{align*}
We can then write $\mu$ as a conditional binomial distribution:
\begin{align*}
    \mu(x_0, k) & := \mathbb{P}_\mu\left(X_0=x_0, \sum_{v=1}^\kappa X_v = k\right).                                              \\
    \intertext{Writing as conditional and letting $K:=\sum_{v=1}^\kappa X_v$,}
                & = \mathbb{P}_\mu(X_0=x_0)\mathbb{P}_\mu\left(K=k | X_0=x_0\right)                                              \\
                & = \mu_0(x_0) {\kappa\choose k}\mathbb{P}_\mu(X_v=+ | X_0=x_0)^k(1-\mathbb{P}_\mu(X_v=+ | X_0=x_0))^{\kappa-k}.
\end{align*}
Hence,
\begin{align*}
    \mu(+, k) & = \mu_0(+){\kappa\choose k}\left(1-\nu_\mu^-\right)^k\left(\nu_\mu^-\right)^{\kappa-k}                                                                \\
    \mu(-, k) & = [1-\mu_0(+)]{\kappa\choose k}\left[\frac{\nu_\mu^-\mu_0(+)}{1-\mu_0(+)}\right]^k\left[\frac{1-(1+\nu_\mu^-)\mu_0(+)}{1-\mu_0(+)}\right]^{\kappa-k}.
\end{align*}
Optimizing with respect to $\mu_0(+)$ and $\nu_\mu^-$ should give the desired results.

\newpage

\section{Jun 19 - Jun 24}

\subsection{Numerical Solution to Edward-Anderson}

\subsubsection{Formulation}

For each of the $\kappa$ vertices, there are 4 possible states for $(y_{0v}, x_v)$: $(-, -), (-, +), (+, -), (+, +)$.
They are labelled by an index in ascending order (0 through 3).

To label each state that the tree can take, we describe it as $(x_0, k_0, k_1, k_2, k_3)$, where $x_0$ is the spin-state of the root vertex,
and $k_\ell$ is the number of vertices in the edge-vertex state of $\ell$ ($\ell=0:3$). Since we have $\kappa$ vertices in total, $\kappa=k_0+k_1+k_2+k_3$.
More compactly, we can enumerate each distinct choice of $(k_0, k_1, k_2, k_3)$ that sums to $\kappa$ with an index $i$. This will be recorded in
a matrix $C$, where $C_{i\ell}$ is the number of vertices in state $\ell$ for the $i$-th edge-vertex configuration.

The following is an example for $\kappa=2$:

\begin{equation*}
    \begin{bmatrix}
        0 & 0 & 0 & 2 \\
        0 & 0 & 1 & 1 \\
        0 & 0 & 2 & 0 \\
        0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 \\
        1 & 0 & 0 & 1 \\
        1 & 0 & 1 & 0 \\
        0 & 2 & 0 & 0 \\
        1 & 1 & 0 & 0 \\
        2 & 0 & 0 & 0
    \end{bmatrix}
\end{equation*}
Note that $i$ can take ${\kappa+3 \choose 3}$ values. This is equivalent to arranging $\kappa$ symbols and $4-1$ separators:
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=5cm]{img/SCR-20230619-ujrs.png}
%     % \caption{https://tjyusun.com/mat202/sec-balls-in-bins.html#:~:text=of\%20size\%20is-,.,distribute\%20indistinguishable\%20balls\%20into\%20bins.}
% \end{figure}

Hence, for the Hamiltonian,
\begin{align*}
    \mathcal{H}(x_0, i) = \frac\beta2x_0(C_{i0} - C_{i1} - C_{i2} + C_{i3}) + Bx_0
\end{align*}
We represent the distribution as $\mu_{x_0, i}$, where $\sum_{x_0\in \mathcal{X}}\sum_{i=1}^{\kappa+3 \choose 3} \mu_{x_0, i} = 1$.
The distribution on the edge becomes
\begin{align*}
    \pi_\mu(x_0, y_{0v}, x_v) & = \frac1\kappa\mathbb{E}_\mu\left[\sum_{v\in\mathcal{N_0(\tau)}}\mathbf{1}_{\{X_0=x_0, Y_{0v}=y_{0v}, X_v=x_v\}}\right] \\
    \pi_\mu(x_0, \ell)        & = \frac1\kappa\sum_{i=1}^{\kappa+3 \choose 3} C_{i\ell}\mu_{x_0, i}
\end{align*}
where $\ell$ corresponds to $(y_{0v}, x_v)$.

This means that the admissibility constraint
\begin{equation*}
    \pi_\mu(x_0, y_{0v}, x_v) = \pi_\mu(x_v, y_{0v}, x_0) \quad \forall x_0, y_{0v}, x_v
\end{equation*}
reduces to two equalities:
\begin{equation*}
    \begin{cases}
        \pi_\mu(+, -, -) & = \pi_\mu(-, -, +) \\
        \pi_\mu(+, +, -) & = \pi_\mu(-, +, +)
    \end{cases}
    \Rightarrow
    \begin{cases}
        \pi_\mu(+, 0) & = \pi_\mu(-, 1) \\
        \pi_\mu(+, 2) & = \pi_\mu(-, 3)
    \end{cases}
\end{equation*}

Optimizations are performed with initial condition $\eta$ and objective function tolerance $10^{-10}$ with SLSQP in SciPy.

\subsubsection{Conditional independence}

One of the objects of interest is the joint conditional distribution of two distinct edge-vertex pairs given the root:
\begin{equation*}
    \mathbb{P}_\mu\left[(X_v, Y_{0v}) = (x_v, y_{0v}), (X_{v'}, Y_{0v'}) = (x_{v'}, y_{0v'}) | X_0 = x_0\right]
\end{equation*}
Due to the permutation symmetry on the leaf nodes, it is simpler to calculate the joint distribution on all edge-vertex
pairs and take the marginal (This is not necessary for $\kappa=2$). Say
\begin{equation*}
    \nu_\mu[(x_1, y_{01}), \ldots, (x_\kappa, y_{0\kappa})| x_0] = \mathbb{P}_\mu\left[(X_1, Y_{01}) = (x_1, y_{01}), \ldots,  (X_{\kappa}, Y_{0\kappa}) = (x_{\kappa}, y_{0\kappa}) | X_0 = x_0\right]
\end{equation*}
Similar to Ising, to get $\nu_\mu$, we only need to remove multiplicity, this time given by a multinomial coefficient:
\begin{equation*}
    \nu_\mu[(x_1, y_{01}), \ldots, (x_\kappa, y_{0\kappa})| x_0] = \frac{1}{{\kappa \choose {k_0\ k_1\ k_2\ k_3}}\mu_0(x_0)}\mu_{x_0, k_0, k_1, k_2, k_3}
\end{equation*}
where $k_l$ is the number of leaves with edge-vertex $(x_v, y_{0v})$ in state $l$, $\mu_0$ is the marginal distribution on the root with $\mu_0(x_0) = \sum_{i=1}^{{\kappa+3\choose 3}} \mu_{x_0, i}$, and the multinomial coefficient
\begin{equation*}
    {\kappa \choose {k_0\ k_1\ k_2\ k_3}} = \frac{\kappa!}{k_0!k_1!k_2!k_3!}.
\end{equation*}

We hypothesize that vertex-edge marks are conditionally independent given the root mark, i.e.
\begin{equation*}
    \nu_\mu[(x_1, y_{01}), \ldots, (x_\kappa, y_{0\kappa})| x_0] = \prod_{v=1}^\kappa \nu_\mu^{v|0} [(x_v, y_{0v}) | x_0]
\end{equation*}
where $\nu_\mu^{v|0} [(x_v, y_{0v}) | x_0]$ is the marginal conditional distribution.

To compare these two, we name the conditional dependent form $\check{\nu}_\mu = \prod_{v=1}^\kappa \nu_\mu^{v|0}$. We want to prove that $\nu_\mu = \check{\nu}_\mu$. We will compare them with relative entropy $H(\nu_\mu \| \check{\nu}_\mu)$. $\nu_\mu = \check{\nu}_\mu$ if and only if $H(\nu_\mu \| \check{\nu}_\mu) = 0$.

\newpage

Indeed, numerically, $H(\nu_\mu \| \check{\nu}_\mu)$ is close to $0$ for all $\beta$ and $B$.
\begin{figure}[h]
    \centering
    \includegraphics[width=7.5cm]{img/EA_x1_x2_RelEntr_x0=-1.png}
    \includegraphics[width=7.5cm]{img/EA_x1_x2_RelEntr_x0=1.png}
    \caption{For Edward-Anderson model $\kappa=2$, the conditional joint distribution of edge-vertex marks given the root marks $\nu_\mu$
        has a zero distance w.r.t. the conditional independent form $\check{\nu}_\mu$. This is confirmed with different values of $\beta$ and $B$.}
    \label{Fig.EA-x1-x2-given-x0-rel-entr}
\end{figure}

\subsubsection{Root mark distribution with respect to external field}

\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{img/EA_x0_B.png}
    \caption{For Edward-Anderson model $\kappa=2$, the distribution of the root mark is only dependent on $B$, and not dependent on $\beta$. }
    \label{Fig.EA-x0-B}
\end{figure}

Getting the marginal distribution on the root, write the distribution in the form
\begin{equation*}
    \mu_0(x_0) = \frac1Z\exp[f(\beta, B)x_0]
\end{equation*}
where $f(\beta, B)$ is some function of the parameters.

Therefore, to extract the information, we have that
\begin{equation*}
    \log \frac{\mu_0(+)}{1 - \mu_0(+)} = 2f(\beta, B).
\end{equation*}
In fact, assuming that the distribution is not dependent on $\beta$, $f$ should not be dependent on $\beta$.
\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{img/EA_x0_B_logisitic_kappa_2.png}
    \caption{For Edward-Anderson model $\kappa=2$, we see that $2f(\beta, B) = 2B$, thus giving a distribution in the form $\mu_0(x_0) = \frac{e^{Bx_0}}{e^B + e^{-B}}$. }
    \label{Fig.EA-x0-B-logisitic}
\end{figure}

If we increase $\kappa$, we see similar results, but with slightly higher uncertainty (Fig.\ref{Fig.EA-x0-B-logisitic-change-kappa}).
zero
\begin{figure}[h]
    \centering
    \includegraphics[width=7.5cm]{img/EA_x0_B_logisitic_kappa_3.png}
    \includegraphics[width=7.5cm]{img/EA_x0_B_logisitic_kappa_4.png}
    \includegraphics[width=7.5cm]{img/EA_x0_B_logisitic_kappa_5.png}
    \includegraphics[width=7.5cm]{img/EA_x0_B_logisitic_kappa_6.png}
    \caption{For Edward-Anderson model with $\kappa=3, 4, 5, 6$, we see that the root mark distribution is roughly the same. }
    \label{Fig.EA-x0-B-logisitic-change-kappa}
\end{figure}
\newpage
Since we cannot impose tolerance on the optimized vector, we see that the optimization algorithm struggles to get the ones with
higher $\kappa$ values and higher $|B|$ values. Higher $B$ values tend to cause multiscale values in the distribution $\mu$, i.e.
some values in $\mu$ are orders of magnitude larger than others. For example, for $\kappa=6$, $B=2$, we have values as large as $0.396$
and as small as $2.43e-12$. This, combined with the much higher dimensionality of $\mu$ ($\mathbb{R}^{168}$ for
$\kappa=2$) may have contributed to the higher uncertainties. However, the prediction is likely still consistent with $f(\beta, B) = B$.

\newpage
\subsection{Analytical solution to Ising}

For $\mu$, we switch to a different convention, where $\mu(x_0, k)$ represents the probability that root vertex is in $x_0$ and there are
$k$ leaf nodes in the same spin as the root vertex.
We can rewrite the Hamiltonian:
\begin{equation*}
    \mathcal{H}(x_0, k) = \frac\beta2(2k-\kappa) + Bx_0
\end{equation*}
Assuming that the distribution $\mu$ has leaf vertex that are conditionally independent on the root vertex, we can write the distribution as
\begin{equation*}
    \mu(x_0, k) = \mu_0(x_0){\kappa \choose k}\left[\pi_\mu^{v|0}(x_0|x_0)\right]^k\left[\pi_\mu^{v|0}(-x_0|x_0)\right]^{\kappa - k}
\end{equation*}
where $\mu_0(x_0)=\sum_{k=0}^\kappa \mu(x_0, k)$ is the marginal distribution on the root vertex, and
$\pi_\mu^{v|0}(x_v|x_0) = \pi_\mu(x_0, x_v) / \mu_0(x_0)$ is the conditional distribution on one of the leaf vertices.

Define $\mu_0^+ = \mu_0(+)$, $\mu_0^- = \mu_0(+)$, $\gamma_\mu^+=\pi_\mu^{v|0}(+|+)$, $\gamma_\mu^-=\pi_\mu^{v|0}(-|-)$. We have
\begin{equation*}
    \begin{aligned}
        \mu_0^+ & = \sum_{k=0}^\kappa \mu(+, k) \\
        \mu_0^- & = \sum_{k=0}^\kappa \mu(-, k)
    \end{aligned}
    \quad\quad\quad
    \begin{aligned}
        \gamma_\mu^+ & = \frac{1}{\kappa\mu_0^+} \sum_{k=0}^\kappa k \mu(+, k) \\
        \gamma_\mu^- & = \frac{1}{\kappa\mu_0^-} \sum_{k=0}^\kappa k \mu(-, k)
    \end{aligned}
\end{equation*}
and
\begin{align*}
    \mu(+, k) = \mu_0^+{\kappa \choose k}\left(\gamma_\mu^+\right)^k\left(1 - \gamma_\mu^+\right)^{\kappa - k} \\
    \mu(-, k) = \mu_0^-{\kappa \choose k}\left(\gamma_\mu^-\right)^k\left(1 - \gamma_\mu^-\right)^{\kappa - k}
\end{align*}
For the distribution to be normalized, $\mu_0^++\mu_0^-=1$, and the symmetry constraint is
\begin{equation*}
    \mu_0^+(1-\gamma_\mu^+) = \mu_0^-(1-\gamma_\mu^-)
\end{equation*}

The objective function (using I-Hsun's convention) is
\begin{equation*}
    V = \mathbb{E}_\mu \mathcal{H}(X_0, \tau) - \left\{H(\mu_0\|\eta_0) + \frac12\left[H(\mu\|\check{\mu}) + H(\mu\|\hat{\mu})\right]\right\}
\end{equation*}
Simplifying each term:
\begin{align*}
      & \mathbb{E}_\mu \mathcal{H}(X_0, \tau)                                                       \\
    = & \sum_{k=0}^\kappa \mu(+, k)\mathcal{H}(+, k) + \sum_{k=0}^\kappa \mu(-, k)\mathcal{H}(-, k) \\
    = & \sum_{k=0}^\kappa \mu(+, k)\left[\frac\beta2 (2k-\kappa) + B\right] +
    \sum_{k=0}^\kappa \mu(-, k)\left[\frac\beta2 (2k-\kappa) - B\right]                             \\
    = & \beta \left[\sum_{k=0}^\kappa k\mu(+, k) + \sum_{k=0}^\kappa k\mu(-, k) \right] -
    \frac\beta2 \kappa\left[\sum_{k=0}^\kappa \mu(+, k) + \sum_{k=0}^\kappa \mu(-, k)\right] +
    B\left[\sum_{k=0}^\kappa \mu(+, k) - \sum_{k=0}^\kappa \mu(-, k)\right]                         \\
    = & \beta \kappa\left(\mu_0^+\gamma_\pi^+ + \mu_0^-\gamma_\pi^-\right)
    + B\left(\mu_0^+ - \mu_0^-\right) - \frac{\beta\kappa}{2}
\end{align*}
Continuing,
\begin{align*}
    H(\mu_0\|\eta_0) = \mu_0^+\log\frac{\mu_0^+}{\eta_0^+} + \mu_0^-\log\frac{\mu_0^-}{\eta_0^-}
\end{align*}
And since $\mu$ is assumed to be conditionally independent on $X_0$, we know that $H(\mu\|\hat{\mu}) = 0$.

On the other hand,
\begin{align*}
      & H(\mu \| \check{\mu})                                                                                   \\
    = & \sum_{k=0}^\kappa \mu(+, k)\log\frac{\mu(+, k)}{\check{\mu}(+, k)}
    + \sum_{k=0}^\kappa \mu(-, k)\log\frac{\mu(-, k)}{\check{\mu}(-, k)}                                        \\
    = & \sum_{k=0}^\kappa \mu(+, k)\log\frac{
    \mu_0^+{\kappa \choose k}\left(\gamma_\mu^+\right)^k\left(1 - \gamma_\mu^+\right)^{\kappa - k}
    }{
    \mu_0^+{\kappa \choose k}\left(\mu_0^+\right)^k\left(\mu_0^-\right)^{\kappa - k}}
    + \sum_{k=0}^\kappa \mu(-, k)\log\frac{
    \mu_0^-{\kappa \choose k}\left(\gamma_\mu^-\right)^k\left(1 - \gamma_\mu^-\right)^{\kappa - k}
    }{
    \mu_0^-{\kappa \choose k}\left(\mu_0^-\right)^k\left(\mu_0^+\right)^{\kappa - k}}                           \\
    = & \sum_{k=0}^\kappa \mu(+, k)\log\frac{
        \left(\gamma_\mu^+\right)^k\left(1 - \gamma_\mu^+\right)^{\kappa - k}
    }{
        \left(\mu_0^+\right)^k\left(\mu_0^-\right)^{\kappa - k}}
    + \sum_{k=0}^\kappa \mu(-, k)\log\frac{
        \left(\gamma_\mu^-\right)^k\left(1 - \gamma_\mu^-\right)^{\kappa - k}
    }{
    \left(\mu_0^-\right)^k\left(\mu_0^+\right)^{\kappa - k}}                                                    \\
    = & \sum_{k=0}^\kappa \mu(+, k)\left[\log\frac{
            \left(\gamma_\mu^+\right)^k\left(1 - \gamma_\mu^+\right)^{-k}
        }{
            \left(\mu_0^+\right)^k\left(\mu_0^-\right)^{-k}}
    + \log \frac{\left(1 - \gamma_\mu^+\right)^\kappa}{\left(\mu_0^-\right)^\kappa}\right]                      \\
      & + \sum_{k=0}^\kappa \mu(-, k)\left[\log\frac{
            \left(\gamma_\mu^-\right)^k\left(1 - \gamma_\mu^-\right)^{-k}
        }{
            \left(\mu_0^-\right)^k\left(\mu_0^+\right)^{-k}}
    + \log\frac{\left(1 - \gamma_\mu^-\right)^\kappa}{\left(\mu_0^+\right)^\kappa}\right]                       \\
    = & \sum_{k=0}^\kappa \mu(+, k)\left[k\log\frac{\gamma_\mu^+ \mu_0^-}{\mu_0^+\left(1 - \gamma_\mu^+\right)}
        + \kappa\log\frac{1 - \gamma_\mu^+}{\mu_0^-}\right]
    + \sum_{k=0}^\kappa \mu(-, k)\left[k\log\frac{\gamma_\mu^- \mu_0^+}{\mu_0^-\left(1 - \gamma_\mu^-\right)}
    + \kappa\log\frac{1 - \gamma_\mu^-}{\mu_0^+}\right]                                                         \\
    = & \kappa\mu_0^+\gamma_\mu^+\log\frac{\gamma_\mu^+ \mu_0^-}{\mu_0^+\left(1 - \gamma_\mu^+\right)}
    + \kappa\mu_0^+\log\frac{1 - \gamma_\mu^+}{\mu_0^-}
    + \kappa\mu_0^-\gamma_\mu^-\log\frac{\gamma_\mu^- \mu_0^+}{\mu_0^-\left(1 - \gamma_\mu^-\right)}
    + \kappa\mu_0^-\log\frac{1 - \gamma_\mu^-}{\mu_0^+}
    \intertext{According to the symmetry constraint, $\mu_0^+(1-\gamma_\mu^+) = \mu_0^-(1-\gamma_\mu^-)$, so}
    = & \kappa\mu_0^+\gamma_\mu^+\log\frac{\gamma_\mu^+ \mu_0^-}{\mu_0^-\left(1 - \gamma_\mu^-\right)}
    + \kappa\mu_0^+\log\frac{1 - \gamma_\mu^+}{\mu_0^-}
    + \kappa\mu_0^-\gamma_\mu^-\log\frac{\gamma_\mu^- \mu_0^+}{\mu_0^+\left(1 - \gamma_\mu^+\right)}
    + \kappa\mu_0^-\log\frac{1 - \gamma_\mu^-}{\mu_0^+}                                                         \\
    = & \kappa\mu_0^+\gamma_\mu^+\log\frac{\gamma_\mu^+}{1 - \gamma_\mu^-}
    + \kappa\mu_0^+\log\frac{1 - \gamma_\mu^+}{\mu_0^-}
    + \kappa\mu_0^-\gamma_\mu^-\log\frac{\gamma_\mu^-}{1 - \gamma_\mu^+}
    + \kappa\mu_0^-\log\frac{1 - \gamma_\mu^-}{\mu_0^+}                                                         \\
\end{align*}

\newpage
We can rewrite the objective function:
\begin{align*}
    V = & \beta \kappa\left(\mu_0^+\gamma_\pi^+ + \mu_0^-\gamma_\pi^-\right)
    + B\left(\mu_0^+ - \mu_0^-\right)
    - \mu_0^+\log\frac{\mu_0^+}{\eta_0^+} - \mu_0^-\log\frac{\mu_0^-}{\eta_0^-}      \\
        & - \frac\kappa2\mu_0^+\gamma_\mu^+\log\frac{\gamma_\mu^+}{1 - \gamma_\mu^-}
    - \frac\kappa2\mu_0^+\log\frac{1 - \gamma_\mu^+}{\mu_0^-}
    - \frac\kappa2\mu_0^-\gamma_\mu^-\log\frac{\gamma_\mu^-}{1 - \gamma_\mu^+}
    - \frac\kappa2\mu_0^-\log\frac{1 - \gamma_\mu^-}{\mu_0^+}
\end{align*}
and the corresponding Lagrangian is
\begin{equation*}
    \tilde{V} = V + \lambda(\mu_0^+(1-\gamma_\mu^+) - \mu_0^-(1-\gamma_\mu^-)) + \gamma (\mu_0^+ + \mu_0^- - 1)
\end{equation*}
Solving for $\gamma_\mu^+$:
\begin{align*}
    \partial_{\gamma_\mu^+} \tilde{V} = \beta\kappa\mu_0^+ -\frac\kappa2\mu_0^+\left(\log\frac{\gamma_\mu^+}{1-\gamma_\mu^-} + 1\right)
    + \frac\kappa2\frac{\mu_0^+}{1-\gamma_\mu^+} - \frac\kappa2\mu_0^-\gamma_\mu^-\frac{1}{1-\gamma_\mu^+}-\lambda\mu_0^+ & = 0 \\
    2\beta - \left(\log\frac{\gamma_\mu^+}{1-\gamma_\mu^-} + 1\right)
    + \frac{1}{1-\gamma_\mu^+} - \frac{\mu_0^-}{\mu_0^+}\frac{\gamma_\mu^-}{1-\gamma_\mu^+}-\frac{2\lambda}{\kappa}       & = 0 \\
    \intertext{by the symmetry constraint, }
    2\beta - \left(\log\frac{\gamma_\mu^+}{1-\gamma_\mu^-} + 1\right)
    + \frac{1}{1-\gamma_\mu^+} - \frac{\mu_0^-}{\mu_0^-}\frac{\gamma_\mu^-}{1-\gamma_\mu^-}-\frac{2\lambda}{\kappa}       & = 0 \\
    2\beta - \left(\log\frac{\gamma_\mu^+}{1-\gamma_\mu^-} + 1\right)
    + \frac{1}{1-\gamma_\mu^+} - \frac{\gamma_\mu^-}{1-\gamma_\mu^-}-\frac{2\lambda}{\kappa}                              & = 0
\end{align*}
Similarly,
\begin{equation*}
    2\beta - \left(\log\frac{\gamma_\mu^-}{1-\gamma_\mu^+} + 1\right)
    + \frac{1}{1-\gamma_\mu^-} - \frac{\gamma_\mu^+}{1-\gamma_\mu^+}+\frac{2\lambda}{\kappa}= 0
\end{equation*}
We should be able to solve for $\gamma_\mu^+$ and $\gamma_\mu^-$ with these two equations.

\newpage

\section{Jun 25 - 30}

\subsection{Edward-Anderson (Reduced) Analytical}

For the Edward-Anderson Model, as discussed above, we model the edge-vertex configurations with counts $(k_{y, x})_{y\in\mathcal{Y}, x\in\mathcal{X}}$, where $k_{y, x}$ counts the number of edge-vertex pairs that have edge-mark of $y$ and vertex mark of $x$. This has an added constraint that
\begin{equation}
    \sum_{y\in\mathcal{Y}}\sum_{x\in\mathcal{X}} k_{y, x} = \kappa
\end{equation}
we can define a vector $\mathbf{k}\in \mathcal{K}$ such that $\mathcal{K} = \{(k_{y, x})_{y\in\mathcal{Y}, x\in\mathcal{X}} : \sum_{y\in\mathcal{Y}}\sum_{x\in\mathcal{X}} k_{y, x} = \kappa\}$ to represent the full state.

In the case that $\mathcal{Y} = \{-1, 1\}$, we can rewrite
\begin{equation}
    \mathcal{H}(x_0, \mathbf{k}) = \frac\beta2 x_0 (k_{++} - k_{+-} - k_{-+} + k_{--}) + Bx_0 \quad\forall \mathbf{k}\in\mathbb{K}
\end{equation}
And $\eta$ is:
\begin{equation*}
    \eta(x_0, \mathbf{k}) = {\kappa \choose \mathbf{k}} 2^{-(2\kappa+1)} \quad\forall \mathbf{k}\in\mathbb{K}
\end{equation*}
where ${\kappa \choose \mathbf{k}} = {\kappa \choose k_{++}\ k_{+-}\ k_{-+}\ k_{--}} = \frac{\kappa!}{k_{++}!k_{+-}!k_{-+}!k_{--}!}$ is the
multinomial coefficient.

Additionally, the edge distribution becomes
\begin{equation}
    \pi_\mu(x_0, y_{0v}, x_v) = \frac1\kappa \sum_{\mathbf{k}\in \mathcal{K}} k_{y_{0v}, x_v} \mu(x_0, \mathbf{k})
\end{equation}
Our objective function is
\begin{equation}
    V(\beta, B, \kappa, \mu) = \mathbb{E}_\mu \mathcal{H}(\tau, X, Y) - H(\mu \| \eta) + \frac\kappa2 H(\pi_\mu \| \pi_\eta)
\end{equation}
Rewriting each term:
\begin{align*}
    \mathbb{E}_\mu \mathcal{H}(\tau, X, Y)                                                                                                  = & \sum_{\mathbf{k}\in\mathcal{K}}[\mu(+, \mathbf{k}) - \mu(-, \mathbf{k})]\left[\frac\beta2(k_{++} - k_{+-} - k_{-+} + k_{--}) + B\right]                                                                                                                                             \\
    =                                                                                                                                         & \sum_{x_0\in\mathcal{X}}\sum_{y_{0v}\in\mathcal{Y}}\sum_{x_v\in\mathcal{X}}x_0 y_{0v} x_v\sum_{\mathbf{k}\in\mathcal{K}} k_{y_{0v}, x_v} \mu(x_0, \mathbf{k}) + B\left[\sum_{\mathbf{k}\in\mathcal{K}}\mu(+, \mathbf{k}) - \sum_{\mathbf{k}\in\mathcal{K}}\mu(-, \mathbf{k})\right]
\end{align*}
and for the relative entropy terms:
\begin{align*}
    H(\mu \| \eta) & = \sum_{x_0\in\mathcal{X}}\sum_{\mathbf{k}\in\mathcal{K}}\mu(x_0, \mathbf{k})\log\mu(x_0, \mathbf{k})
    - \sum_{x_0\in\mathcal{X}}\sum_{\mathbf{k}\in\mathcal{K}}\mu(x_0, \mathbf{k})\log\eta(x_0, \mathbf{k})                 \\
                   & = \sum_{x_0\in\mathcal{X}}\sum_{\mathbf{k}\in\mathcal{K}}\mu(x_0, \mathbf{k})\log\mu(x_0, \mathbf{k})
    - \sum_{x_0\in\mathcal{X}}\sum_{\mathbf{k}\in\mathcal{K}}\mu(x_0, \mathbf{k})\log{\kappa \choose \mathbf{k}}
    + (2\kappa + 1)\log 2
\end{align*}
and
\begin{align*}
      & H(\pi_\mu \| \pi_\eta)                                                                                \\
    = & \sum_{x_0\in\mathcal{X}}\sum_{y_{0v}\in\mathcal{Y}}\sum_{x_v\in\mathcal{X}}
    \pi_\mu(x_0, y_{0v}, x_v)\log \pi_\mu(x_0, y_{0v}, x_v) -
    \sum_{x_0\in\mathcal{X}}\sum_{y_{0v}\in\mathcal{Y}}\sum_{x_v\in\mathcal{X}}
    \pi_\mu(x_0, y_{0v}, x_v)\log \pi_\eta(x_0, y_{0v}, x_v)                                                  \\
    = & \sum_{x_0\in\mathcal{X}}\sum_{y_{0v}\in\mathcal{Y}}\sum_{x_v\in\mathcal{X}}
    \left[\frac1\kappa\sum_{\mathbf{k}\in\mathcal{K}}k_{y_{0v}, x_v}\mu(x_0, \mathbf{k})\right]
    \log \left[\frac1\kappa\sum_{\mathbf{k}\in\mathcal{K}}k_{y_{0v}, x_v}\mu(x_0, \mathbf{k})\right] + \log 8 \\
    = & \frac1\kappa\sum_{x_0\in\mathcal{X}}\sum_{y_{0v}\in\mathcal{Y}}\sum_{x_v\in\mathcal{X}}
    \left[\sum_{\mathbf{k}\in\mathcal{K}}k_{y_{0v}, x_v}\mu(x_0, \mathbf{k})\right]
    \log \left[\sum_{\mathbf{k}\in\mathcal{K}}k_{y_{0v}, x_v}\mu(x_0, \mathbf{k})\right] + \log \frac8\kappa
\end{align*}
Hence, the objective function becomes
\begin{equation}
    \begin{aligned}
          & V(\beta, B, \kappa, \mu)                                                                                                                                                                                                                                                            \\
        = & \sum_{x_0\in\mathcal{X}}\sum_{y_{0v}\in\mathcal{Y}}\sum_{x_v\in\mathcal{X}}x_0 y_{0v} x_v\sum_{\mathbf{k}\in\mathcal{K}} k_{y_{0v}, x_v} \mu(x_0, \mathbf{k}) + B\left[\sum_{\mathbf{k}\in\mathcal{K}}\mu(+, \mathbf{k}) - \sum_{\mathbf{k}\in\mathcal{K}}\mu(-, \mathbf{k})\right] \\
          & - \sum_{x_0\in\mathcal{X}}\sum_{\mathbf{k}\in\mathcal{K}}\mu(x_0, \mathbf{k})\log\mu(x_0, \mathbf{k})
        + \sum_{x_0\in\mathcal{X}}\sum_{\mathbf{k}\in\mathcal{K}}\mu(x_0, \mathbf{k})\log{\kappa \choose \mathbf{k}}                                                                                                                                                                            \\
          & + \frac12\sum_{x_0\in\mathcal{X}}\sum_{y_{0v}\in\mathcal{Y}}\sum_{x_v\in\mathcal{X}}
        \left[\sum_{\mathbf{k}\in\mathcal{K}}k_{y_{0v}, x_v}\mu(x_0, \mathbf{k})\right]
        \log \left[\sum_{\mathbf{k}\in\mathcal{K}}k_{y_{0v}, x_v}\mu(x_0, \mathbf{k})\right]                                                                                                                                                                                                    \\
          & - (2\kappa + 1)\log 2 + \frac\kappa2 \log \frac8\kappa
    \end{aligned}
    \label{Eq.Objective-expanded}
\end{equation}
Similar to I-Hsun's notes, we notice that besides line 2 of (\ref{Eq.Objective-expanded}):
\begin{equation}
    - \sum_{x_0\in\mathcal{X}}\sum_{\mathbf{k}\in\mathcal{K}}\mu(x_0, \mathbf{k})\log\mu(x_0, \mathbf{k})
    + \sum_{x_0\in\mathcal{X}}\sum_{\mathbf{k}\in\mathcal{K}}\mu(x_0, \mathbf{k})\log{\kappa \choose \mathbf{k}}
    \label{Eq.EA-intermediate-obj}
\end{equation}
everything else is dependent on $\sum_{\mathbf{k}\in\mathcal{K}} k_{y_{0v}, x_v}\mu(x_0, \mathbf{k})$ and
$\sum_{\mathbf{k}\in\mathcal{K}}\mu(x_0, \mathbf{k})$, the latter a linear combination of the former. We optimize for $\mu$
subject to the constraints: $\sum_{\mathbf{k}\in\mathcal{K}} k_{y_{0v}, x_v}\mu(x_0, \mathbf{k}) = m^{x_0, y_{0v}, x_v}$,
$\sum_{\mathbf{k}\in\mathcal{K}}\mu(+, \mathbf{k}) + \sum_{\mathbf{k}\in\mathcal{K}}\mu(-, \mathbf{k}) = 1$, and
$\sum_{\mathbf{k}\in\mathcal{K}}\mu(+, \mathbf{k}) - \sum_{\mathbf{k}\in\mathcal{K}}\mu(-, \mathbf{k}) = m$.

Solving the problem gives the following solution:
\begin{equation}
    \begin{cases}
        \mu(+, \mathbf{k}) = \displaystyle{\frac{e^\gamma}{Z}{\kappa\choose\mathbf{k}} \prod_{y_\in\mathcal{Y}, x\in\mathcal{X}}\left(q^{+,y,x}\right)^{k_{y,x}}}    \\
        \mu(-, \mathbf{k}) = \displaystyle{\frac{e^{-\gamma}}{Z}{\kappa\choose\mathbf{k}} \prod_{y_\in\mathcal{Y}, x\in\mathcal{X}}\left(q^{-,y,x}\right)^{k_{y,x}}} \\
    \end{cases}
    \label{Eq.dist-form1}
\end{equation}
where $Z = e^\gamma\left(\sum_{y_\in\mathcal{Y}, x\in\mathcal{X}}q^{+,y,x}\right)^\kappa + e^{-\gamma}\left(\sum_{y_\in\mathcal{Y}, x\in\mathcal{X}}q^{-,y,x}\right)^\kappa$ is the normalizing constant.

For the distribution to be symmetric, we need
\begin{align}
    \pi_\mu(+, +, -) = \pi_\mu(-, +, +) \label{Eq.EA-reduced-Sym-constr1} \\
    \pi_\mu(+, -, -) = \pi_\mu(-, -, +) \label{Eq.EA-reduced-Sym-constr2}
\end{align}
Substituting (\ref{Eq.dist-form1}) into (\ref{Eq.EA-reduced-Sym-constr1}), we have
\begin{align}
    \sum_{k\in\mathcal{K}}k_{+-}\mu(+, \mathbf{k})                                                       & = \sum_{k\in\mathcal{K}}k_{++}\mu(-, \mathbf{k})                                                         \nonumber                       \\
    e^\gamma\kappa q^{++-} \left(\sum_{{y_\in\mathcal{Y}, x\in\mathcal{X}}}q^{+, y, x}\right)^{\kappa-1} & = e^{-\gamma}\kappa q^{-++} \left(\sum_{{y_\in\mathcal{Y}, x\in\mathcal{X}}}q^{-, y, x}\right)^{\kappa-1} \label{Eq.EA-reduced-Sym-soln}
\end{align}
Using (\ref{Eq.EA-reduced-Sym-soln}) in (\ref{Eq.dist-form1}) gives
\begin{equation}
    \begin{cases}
        \mu(+, \mathbf{k}) = \displaystyle{\frac{q^{-++}\sum_{y, x}q^{+,y,x}}{q^{-++}\sum_{y, x}q^{+,y,x} + q^{++-}\sum_{y, x}q^{-,y,x}}{\kappa\choose\mathbf{k}} \prod_{y_\in\mathcal{Y}, x\in\mathcal{X}}\left(\frac{q^{+,y,x}}{\sum_{y', x'}q^{+,y',x'}}\right)^{k_{y,x}}} \\
        \mu(-, \mathbf{k}) = \displaystyle{\frac{q^{++-}\sum_{y, x}q^{-,y,x}}{q^{-++}\sum_{y, x}q^{+,y,x} + q^{++-}\sum_{y, x}q^{-,y,x}}{\kappa\choose\mathbf{k}} \prod_{y_\in\mathcal{Y}, x\in\mathcal{X}}\left(\frac{q^{-,y,x}}{\sum_{y', x'}q^{-,y',x'}}\right)^{k_{y,x}}}
    \end{cases}
    \label{Eq.dist-form2}
\end{equation}
This is a multinomial distribution. It means that the distribution has edge-vertex marks conditionally independent of each other given the root mark.

Now consider the constraint (\ref{Eq.EA-reduced-Sym-constr2}). This gives a form similar to (\ref{Eq.EA-reduced-Sym-soln}), and we have
\begin{equation}
    \frac{q^{++-}}{q^{-++}}=\frac{q^{+--}}{q^{--+}}
\end{equation}
In fact, this hints at the (\ref{Eq.dist-form2}) having some redundancy. Let $r=\frac{q^{++-}}{q^{-++}}=\frac{q^{+--}}{q^{--+}}$
and write $\tilde{q}^{-,y,x}=rq^{-,y,x}$, it gets simplified into
\begin{equation}
    \begin{cases}
        \mu(+, \mathbf{k}) = \displaystyle{\frac{\sum_{y, x}q^{+,y,x}}{\sum_{y, x}q^{+,y,x} + \sum_{y, x}\tilde{q}^{-,y,x}}{\kappa\choose\mathbf{k}} \prod_{y_\in\mathcal{Y}, x\in\mathcal{X}}\left(\frac{q^{+,y,x}}{\sum_{y', x'}q^{+,y',x'}}\right)^{k_{y,x}}} \\
        \mu(-, \mathbf{k}) = \displaystyle{\frac{\sum_{y, x}\tilde{q}^{-,y,x}}{\sum_{y, x}q^{+,y,x} + \sum_{y, x}\tilde{q}^{-,y,x}}{\kappa\choose\mathbf{k}} \prod_{y_\in\mathcal{Y}, x\in\mathcal{X}}\left(\frac{\tilde{q}^{-,y,x}}{\sum_{y', x'}\tilde{q}^{-,y',x'}}\right)^{k_{y,x}}}
    \end{cases}
\end{equation}
And if you write $q^{++-} = \tilde{q}^{-++}$ and $q^{+--} = \tilde{q}^{--+}$, the number of independent variables can be reduced to $5$ (which brings it closer to the form we see with Ising model):
\begin{equation}
    \begin{cases}
        \mu(+, \mathbf{k}) = \displaystyle{\frac{\nu^+}{\nu^+ + \nu^-}{\kappa\choose\mathbf{k}} \left(\frac{\nu^{++}}{\nu^+}\right)^{k_{++}}\left(\frac{\nu^{+-}}{\nu^+}\right)^{k{-+}}\left(\frac{\nu^{\times}}{\nu^+}\right)^{k_{+-}}\left(\frac{1}{\nu^+}\right)^{k_{--}}} \\
        \mu(-, \mathbf{k}) = \displaystyle{\frac{\nu^-}{\nu^+ + \nu^-}{\kappa\choose\mathbf{k}} \left(\frac{\nu^{-+}}{\nu^-}\right)^{k_{+-}}\left(\frac{\nu^{--}}{\nu^-}\right)^{k_{--}}\left(\frac{\nu^{\times}}{\nu^-}\right)^{k_{++}}\left(\frac{1}{\nu^-}\right)^{k_{-+}}}
    \end{cases}
\end{equation}
where $\nu^{+, y} = \frac{q^{+, y, +}}{q^{+--}}$, $\nu^{-, y} = \frac{q^{-, y, -}}{\tilde{q}^{--+}}$, $\nu^{\times} = \frac{q^{++-}}{q^{+--}} = \frac{\tilde{q}^{-++}}{\tilde{q}^{--+}}$, $\nu^+=1+\nu^{++}+\nu^\times+\nu^{+-}$, $\nu^-=1+\nu^{-+}+\nu^\times+\nu^{--}$. I think that this notation
can extended to arbitrary number of values of $y$. Namely, for $|\mathcal{X}|=2$, there are $3|\mathcal{Y}| - 1$ independent variables. (For Ising,
$|\mathcal{Y}|=1$, and there are $2$ independent variables. For EA with $|\mathcal{Y}|=2$, there are $5$ independent variables.)

\newpage

\section{Jul 1-4}

\subsection{Edward-Anderson solving for the distribution}

\subsubsection{Conditional Independence on root node}
For the optimization in (\ref{Eq.EA-intermediate-obj}), we will remove the constraint $\mu_0(+) - \mu_0(-)=1$ and without applying the
admissibility constraint, we have
\begin{equation}
    \mu(x, \mathbf{k}) = \frac{1}{{\sum_{x_0}\left(\sum_{y, x'} q^{x_0, y, x'}\right)^\kappa}}{\kappa \choose \mathbf{k}}\prod_{y, x'}(q^{xyx'})^{k_{yx'}}
\end{equation}
Alternatively, we can write
\begin{equation}
    \mu(x, \mathbf{k}) = \frac{\rho^x}{{\sum_{x_0} \rho^{x_0}}}{\kappa \choose \mathbf{k}}\prod_{y, x'}\left(\nu^{xyx'}\right)^{k_{yx'}}
\end{equation}
where $\rho^x = \left(\sum_{y, x'} q^{xyx'}\right)^\kappa$, and $\nu^{xyx'} = (q^x)^{-1}q^{xyx'}$

We can see that
\begin{equation}
    \pi_\mu^{xyx'}:=\pi_\mu(x, y, x') = \frac1\kappa\sum_{\mathbf{k}\in\mathcal{K}}k_{yx'}\mu(x, \mathbf{k}) = \frac{\rho^x}{{\sum_{x_0} \rho^{x_0}}} \nu^{xyx'}
\end{equation}
and
\begin{equation}
    \mu_0^x:=\mu_0(x) = \sum_{\mathbf{k}\in\mathcal{K}} \mu(x, \mathbf{k}) = \frac{\rho^x}{{\sum_{x_0} \rho^{x_0}}}
\end{equation}

With this, we can rewrite
\begin{align}
    H(\mu \| \eta) & = H(\mu_0) + \kappa H_{\pi_\mu}(X_v, Y_{0v} | X_0) + (2\kappa + 1) \log 2 \\
                   & = (\kappa-1)H(\mu_0) - \kappa H(\pi_\mu) + (2\kappa + 1) \log 2
\end{align}
Hence
\begin{equation}
    V(\beta, B, \kappa, \mu) = \mathbb{E}_{\pi_\mu}\left(\frac{\beta\kappa}{2}X_0Y_{0v}X_v + BX_0\right) - (\kappa - 1) H(\mu_0) + \frac\kappa2 H(\pi_\mu) - \left(\frac\kappa2 + 1\right)\log 2
\end{equation}
Applying Lagrange multipliers
\begin{equation}
    \partial \pi_\mu^{xyx'} = \frac{\beta\kappa}{2}xyx' + Bx + (\kappa - 1)\left(\log\mu_0^x + 1\right) - \frac\kappa2\left(\log\pi_\mu^{xyx'} + 1\right) + \lambda + \mathbf{1}_{\{x\neq x'\}} \gamma x = 0
    \label{Eq.EA-LM-pi-mu}
\end{equation}
where $\lambda$ is the normalizer for the $\pi_\mu$ normalized and $\gamma$ for symmetric.

\subsubsection{Attempts at solving (\ref{Eq.EA-LM-pi-mu})}

\newpage

\subsection{Edward-Anderson with multiple edge marks}

We experiment with different distributions of edge mark $Y$. We will try symmetric $Y$, i.e. $\eta_Y(y) = \eta_Y(-y)$.
The probability space will also be evenly-spaced (e.g. $\mathcal{Y}=\{-1, 1\}$, $\mathcal{Y}=\{-1,0, 1\}$, $\mathcal{Y}=\{-3, -1, 1, 3\}$)

We initialize with a vector $\mu$ where each component is iid. Uniform on $[-1, 1)$. They are solved with SLSQP with an objective function tolerance
of $10^{-10}$.

\subsubsection{Conditional Independence}

We consider the distribution on $\mathcal{Y}=\{-1, 0, 1\}$ where $\eta_Y(0)=p$ and $\eta_Y(-1)=\eta_Y(1)=\frac12(1-p)$

We see that conditional independence given the root node is still true.
\begin{figure}[h]
    \centering
    \includegraphics[width=7.5cm]{img/EA_x1_x2_x0=1_3s_p=0.50_kappa=2.png}
    \includegraphics[width=7.5cm]{img/EA_x1_x2_x0=-1_3s_p=0.50_kappa=2.png}
    \includegraphics[width=7.5cm]{img/EA_x1_x2_x0=1_3s_p=0.33_kappa=2.png}
    \includegraphics[width=7.5cm]{img/EA_x1_x2_x0=-1_3s_p=0.33_kappa=2.png}
    \caption{Edward-Anderson with edge marks $\{-1, 0, 1\}$ and $\kappa=2$. $\eta_Y(0)=p$ and $\eta_Y(-1)=\eta_Y(1)=\frac12(1-p)$. Edge-vertex marks are conditionally independent given the root mark.}
    \label{Fig.EA-CI-k2-3s}
\end{figure}

\newpage

Confirming with 5 states. We have $\mathcal{Y}=\{-2, -1, 0, 1, -2\}$ and $\eta_Y(0)=p$, $\eta_Y(\pm 1)=q$, $\eta_Y(\pm 2)=\frac12-\frac12p-q$

\begin{figure}[h]
    \centering
    \includegraphics[width=7.5cm]{img/EA_x1_x2_x0=1_5s_p=0.38_q=0.25_kappa=2.png}
    \includegraphics[width=7.5cm]{img/EA_x1_x2_x0=-1_5s_p=0.38_q=0.25_kappa=2.png}
    \caption{Edward-Anderson with edge marks $\{-2, -1, 0, 1, 2\}$ and $\kappa=2$. $\kappa=2$. $\eta_Y(0)=p$, $\eta_Y(\pm 1)=q$, $\eta_Y(\pm 2)=\frac12-\frac12p-q$. Edge-vertex marks are conditionally independent given the root mark.}
    \label{Fig.EA-CI-k2-5s}
\end{figure}

\newpage

\subsubsection{Root mark distribution}

We can see that for symmetric $\eta_Y$, we consistently get a root mark distribution independent on $\beta$. Again, we plot $\log\frac{\mu_0^+}{\mu_0^-}$:

\begin{figure}[h]
    \centering
    \includegraphics[width=7.5cm]{img/EA_x0_B_logi_kappa_2_3s_p=0.25.png}
    \includegraphics[width=7.5cm]{img/EA_x0_B_logi_kappa_2_3s_p=0.33.png}
    \includegraphics[width=7.5cm]{img/EA_x0_B_logi_kappa_2_3s_p=0.50.png}
    \includegraphics[width=7.5cm]{img/EA_x0_B_logi_kappa_2_3s_p=0.67.png}
    \caption{Edward-Anderson model with edge marks $\{-1, 0, 1\}$ and $\kappa=2$. $\eta_Y(0)=p$ and $\eta_Y(-1)=\eta_Y(1)=\frac12(1-p)$. The root mark distribution is independent of $B$}
    \label{Fig.EA-RM-k2-3s}
\end{figure}

\newpage

The conclusion is consistent with 5 edge states.
\begin{figure}[h]
    \centering
    \includegraphics[width=7.5cm]{img/EA_x0_B_logi_kappa_2_5s_p=0.20_q=0.20.png}
    \includegraphics[width=7.5cm]{img/EA_x0_B_logi_kappa_2_5s_p=0.33_q=0.25.png}
    \includegraphics[width=7.5cm]{img/EA_x0_B_logi_kappa_2_5s_p=0.38_q=0.25.png}
    \includegraphics[width=7.5cm]{img/EA_x0_B_logi_kappa_2_5s_p=0.50_q=0.17.png}
    \caption{Edward-Anderson model with edge marks $\{-2, -1, 0, 1, -2\}$ and $\kappa=2$. $\eta_Y(0)=p$, $\eta_Y(\pm 1)=q$, $\eta_Y(\pm 2)=\frac12-\frac12p-q$. The root mark distribution is indepenident of $B$}
    \label{Fig.EA-RM-k2-5s}
\end{figure}

\newpage
The conclusion does not change with more leaves:
\begin{figure}[h]
    \centering
    \includegraphics[width=7.5cm]{img/EA_x0_B_logi_kappa_3_3s_p=0.50.png}
    \includegraphics[width=7.5cm]{img/EA_x0_B_logi_kappa_4_3s_p=0.50.png}
    \caption{Edward-Anderson model with edge marks $\{-1, 0, 1\}$ and $\kappa=3$. $\eta_Y(0)=p$ and $\eta_Y(-1)=\eta_Y(1)=\frac12(1-p)$. The root mark distribution is independent of $B$}
    \label{Fig.EA-RM-k3-3s}
\end{figure}

Recall that in Ising model, we have a cavity field term that makes the root mark distribution dependent on $\beta$. Why is it the case that by adding
an edge mark makes this term disappear? We notice that Ising model is equivalent to Edward-Anderson with $\eta_Y=\delta_1$. This is not a symmetric
distribution. This leads to the hypothesis that a non-symmetric $\eta$ leads to a root-mark distribution dependent on $\beta$.

We instead consider a more general form of symmetry: $\eta_Y(\bar{Y}+\Delta y) = \eta_Y(\bar{Y}-\Delta y)$, still with evenly-spaced $\mathcal{Y}$.
\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{img/EA_x0_B_logi_kappa_2_3s_p=0.50_symF.png}
    \caption{Edward-Anderson model with edge marks $\{0, 1, 2\}$, $\eta_Y(1)=p$ and $\eta_Y(0)=\eta_Y(2)=\frac12(1-p)$. We see that the root mark distribution is now dependent on $\beta$}
    \label{Fig.EA-RM-k2-3s-asym}
\end{figure}

\newpage

\section{Jul 10-17}

\subsection{Hardcore Ising Numerical}
We now have Hamiltonian
\begin{equation}
    \mathcal{H}(x_0, k) = \beta x_0
\end{equation}
with constraint that
\begin{equation}
    \mu(x_0, k) = 0 \qquad \forall x_0=1, k>0
\end{equation}
The optimization target is the same:
\begin{equation*}
    \mathbb{E}_\mu[\beta X_0] - H(\mu \| \eta) + \frac\kappa2 H(\pi_\mu \| \pi_\eta)
\end{equation*}

For the optimizer, it is easier to deal with a smaller vector and assume the rest are simply 0.
\begin{equation*}
    (\mu_{-0}, \mu_{-1}, \cdots, \mu_{-\kappa}, \mu_{+0})
\end{equation*}

\subsubsection{Conditional Independence}

We observe conditional independence given root mark on small $\kappa$ values:
\begin{figure}[h]
    \centering
    \includegraphics[width=7.5cm]{img/HA_Ising/CI_kappa2.png}
    \includegraphics[width=7.5cm]{img/HA_Ising/CI_kappa3.png}
    \includegraphics[width=7.5cm]{img/HA_Ising/CI_kappa4.png}
    \includegraphics[width=7.5cm]{img/HA_Ising/CI_kappa5.png}
    \caption{In Hardcore Ising model, we still observe conditional independence for $\kappa=2, 3, 4, 5$.}
    \label{Fig.HAIS-CI}
\end{figure}

The conclusion is not changed for higher $\kappa$ values:
\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{img/HA_Ising/CI_kappa20.png}
    \caption{In Hardcore Ising model, we still observe conditional independence for $\kappa=20$.}
    \label{Fig.HAIS-CI-large-kappa}
\end{figure}

\subsubsection{Root mark distribution}

This is the root mark distribution of Hardcore Ising
\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{img/HA_Ising/RM_raw.png}
    \caption{This is the Root mark distribution for Hardcore Ising model. No significant transition is seen at $\kappa=16$}
    \label{Fig.HAIS-RM}
\end{figure}

\newpage

\subsubsection{Edge Distribution}

We see that the root and leaf marks ($\pi_\mu$) are never independent, and only tends to a degenerate independence as $\beta\rightarrow-\infty$.
\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{img/HA_Ising/RL_indep.png}
    \caption{In Hardcore Ising, $X_0$ and $X_v$ tends to independence as $\beta\rightarrow-\infty$, and they tend towards dependence as $\beta\rightarrow\infty$. The top line is at $\log 2$, the maximum mutual information possible with Bernoulli random variables.
        It is when $X_0$ are completely dependent on each other ($X_0=f(X_v)$).}
    \label{Fig.HAIS-RL-indep}
\end{figure}

To see what type of dependence we have, we look at the $\pi_\mu$ distribution:
\begin{figure}[h]
    \centering
    \includegraphics[width=15cm]{img/HA_Ising/RL_indiv.png}
    \caption{$\pi_\mu$ distribution under different $\beta$ values in Hardcore Ising ($\kappa=2$) is shown. Darker means greater value. $X_0=0$ is the top row and $X_v$ is the left column.}
    \label{Fig.HAIS-RL-beta-trends}
\end{figure}

First observe that due to the symmetry constraint, the $\pi_\mu$ distribution is represented by a symmetric matrix. Then, see that $\pi_\mu(1, 1)$ has $0$ probability due to the hardcore constraint.

When $\beta\rightarrow-\infty$, we observe that the distribution tends to have $X_0=X_v=0$. Indeed, this tends towards independence, but it is more
degenerate. When $\beta\rightarrow\infty$, we see that it tends towards $X_0=1-X_v$, preferring the root and leaves to have different marks.
Mark distributions also tend towards Bernoulli(1/2) (Due to the symmetry constraint).

\newpage

\subsection{Hardcore Ising Analytical}

\subsubsection{Conditional Independence}

Let the set of symmetric probability measure on a marked $\kappa$ stars that satisfy the hardcore constraint:
\begin{equation}
    M = \left\{\mu: \mathcal{X} \times [\kappa] \mapsto [0, \infty) \middle| \sum_{x\in\mathcal{X}}\sum_{k=0}^\kappa \mu(x, k) = 1, \pi_\mu(1, 0) = \pi_\mu(1, 0), \mu(1, k) = 0\forall k=1:\kappa\right\}
\end{equation}
The corresponding edge distribution $\pi_\mu$ satisfies
\begin{equation}
    \Delta = \left\{\pi_\mu: \mathcal{X}^2 \mapsto [0, \infty) \middle| \sum_{x, x'\in\mathcal{X}}\pi_\mu(x, x') = 1, \pi_\mu(1, 0) = \pi_\mu(1, 0), \pi_\mu(1, 1) = 0\right\}
\end{equation}

\begin{lemma}
    For probability measure $\mu$ on marked $\kappa$-stars that satisfy the hardcore constraint,
    \begin{enumerate}
        \item the solution of the optimization problem
              \begin{equation*}
                  \psi(\beta, \kappa) = \sup_{\mu\in M} \left\{\mathbb{E}_\mu\left[\beta X_0\right] - \left[H(\mu\|\eta) - \frac\kappa2 H(\pi_\mu\| \pi_\eta)\right]\right\}
              \end{equation*}
              is in the form
              \begin{equation}
                  \begin{cases}
                      \widehat{\mu}(0, k) & = \displaystyle\widehat{\mu}_0^0{\kappa\choose k}\left(\frac{\widehat{\pi}^{00}}{\widehat{\mu}_0^0}\right)^k\left(\frac{\widehat{\pi}^{01}}{\widehat{\mu}_0^0}\right)^{\kappa-k} \\
                      \widehat{\mu}(1, 0) & = \displaystyle\widehat{\mu}_0^1,
                  \end{cases}
              \end{equation}
              where $\widehat{\pi}^{x, x'}\in\Delta$ and $\widehat{\mu}_0^x = \sum_{x'\in\mathcal{X}} \widehat{\pi}^{x, x'}$ is the corresponding root marginal,
        \item and the optimization problem is equivalent to
              \begin{equation*}
                  \psi(\beta, \kappa) = \sup_{\widehat{\pi}\in\Delta}\left[ \mathbb{E}_{\widehat{\mu}_0} \beta X_0 + \frac\kappa2 H(\widehat{\pi}) - (\kappa-1) H(\widehat{\mu}_0) - \log|\mathcal{X}| \right].
                  \label{Eq.HAIS-V-tilde}
              \end{equation*}
    \end{enumerate}
\end{lemma}

Note that
\begin{enumerate}
    \item The edge measure $\pi_{\widehat{\mu}}(x, x') = \widehat{\pi}^{x, x'}$, thus $\widehat{\mu}_0(x)=\widehat{\mu}_0^x$,
    \item the symmetric measure $\widehat{\pi}$ can be parametrized by one parameter, since:
          \begin{equation}
              \widehat{\pi}^{01} = \widehat{\pi}^{10} = \frac12(1-\widehat{\pi}^{00}),
              \label{Eq.HAIS-pi-param}
          \end{equation}
    \item and $\widehat{\mu}\in M$.
\end{enumerate}

\begin{proof}
    It is sufficient to prove that $\widehat{\mu}$ with $\widehat{\pi} = \pi_\mu$ is the optimizer of
    \begin{equation}
        \begin{aligned}
            \text{maximize}   & _{\tilde{\mu}\in M}\qquad \mathbb{E}_{\tilde{\mu}}\left[\beta X_0\right] - \left[H(\tilde{\mu}\|\eta) - \frac\kappa2 H(\pi_{\tilde{\mu}}\| \pi_\eta)\right] \\
            \text{subject to} & \qquad \pi_{\tilde{\mu}} = \pi_\mu
        \end{aligned}
    \end{equation}

    First, rewriting the objective function, which we define as $V$.
    \begin{equation*}
        V(\beta, \kappa, \tilde{\mu}) = \beta\sum_{x\in\mathcal{X}} \tilde{\mu}_0(x) x + H(\tilde{\mu}) + \sum_{x\in\mathcal{X}}\sum_{k=0}^\kappa \tilde{\mu}(x, k)\log{\kappa\choose k} - \frac\kappa2 H(\pi_{\tilde{\mu}}) - \log|\mathcal{X}|
    \end{equation*}
    where $\tilde{\mu}_0(x) = \sum_{k=0}^\kappa\tilde{\mu}(x, k)$.

    Since $\pi_{\tilde{\mu}} = \pi_\mu$ we can rewrite
    \begin{equation}
        V(\beta, \kappa, \tilde{\mu}) = \beta\sum_{x\in\mathcal{X}} \mu_0^x x + H(\tilde{\mu}) + \sum_{x\in\mathcal{X}}\sum_{k=0}^\kappa \tilde{\mu}(x, k)\log{\kappa\choose k} - \frac\kappa2 H(\pi_{\mu}) - \log|\mathcal{X}|
        \label{Eq.HAIS-V-2}
    \end{equation}
    Observe that $V$ is only dependent on $\tilde{\mu}$ in the second and third term after the constraints are applied.

    Solving for this optimization problem (over hardcore support) with constraints
    \begin{equation}
        \begin{aligned}
            \pi_\mu(0, 0) & = \frac1\kappa \sum_{k=0}^\kappa k\tilde{\mu}(0, k)          \\
            \pi_\mu(0, 1) & = \frac1\kappa \sum_{k=0}^\kappa (\kappa-k)\tilde{\mu}(0, k) \\
            \pi_\mu(1, 0) & = \tilde{\mu}(1, 0)
        \end{aligned}
    \end{equation}
    with corresponding Lagrange multipliers $\lambda^{00}$, $\lambda^{01}$, and $\lambda^{10}$.

    This gives the following equations:
    \begin{equation}
        \begin{cases}
            \partial \tilde{\mu}(0, k) & = -\log \tilde{\mu}(0, k) - 1 + \log {\kappa \choose k} + \lambda^{00} k  + \lambda^{01} (\kappa-k) = 0 \\
            \partial \tilde{\mu}(1, 0) & = -\log \tilde{\mu}(0, 1) - 1 + \lambda^{10} \kappa = 0.
        \end{cases}
    \end{equation}
    Let $q^{x, x'} = \exp\left(\lambda^{x, x'} - 1 / \kappa\right)$, we have the following form for $\tilde{\mu}$:
    \begin{equation}
        \begin{cases}
            \tilde{\mu}(0, k) & = \displaystyle{\kappa \choose k} (q^{00})^k (q^{01})^{\kappa - k} \\
            \tilde{\mu}(1, 0) & = \displaystyle (q^{10})^\kappa.
        \end{cases}
    \end{equation}
    We therefore need to solve for
    \begin{equation}
        \begin{aligned}
            \pi_\mu(0, 0) & = q^{00}(q^{00} + q^{01})^{\kappa-1} \\
            \pi_\mu(0, 1) & = q^{01}(q^{00} + q^{01})^{\kappa-1} \\
            \pi_\mu(1, 0) & = (q^{10})^\kappa
        \end{aligned}
        \label{Eq.HAIS-pimu-q}
    \end{equation}
    with
    \begin{equation}
        \begin{aligned}
            \mu_0(0) & = (q^{00} + q^{01})^{\kappa} \\
            \mu_0(1) & = (q^{10})^\kappa.
        \end{aligned}
    \end{equation}

    Hence, the stationary point is
    \begin{equation*}
        \begin{cases}
            \mu(0, k) & = \displaystyle\mu_0(0){\kappa\choose k}\left[\frac{\pi_\mu(0, 0)}{\mu_0(0)}\right]^k\left[\frac{\pi_\mu(0, 1)}{\mu_0(0)}\right]^{\kappa-k} \\
            \mu(1, 0) & = \displaystyle\mu_0(1).
        \end{cases}
    \end{equation*}

    To see that the stationary point is indeed a maximum, notice that in (\ref{Eq.HAIS-V-2}), $H(\mu)$ is concave in probability measure $\mu$ and $\sum_{x\in\mathcal{X}}\sum_{k=0}^\kappa \mu(x, k)\log{\kappa\choose k}$ is linear in $\mu$. This gives that $V$ is concave in $\mu$. Thus, by continuity of $V$, the maximum is unique.

    Since $\widehat{\mu}$ is the unique maximizer of $V(\beta, \kappa, \cdot)$ given any edge distribution $\widehat{\pi}\in\Delta$, any $\mu$ will have
    \begin{equation*}
        V(\beta, \kappa, \mu) \leq V(\beta, \kappa, \widehat{\mu}),
    \end{equation*}
    which means that an optimizer always comes in the form of $\widehat{\mu}$.

    Hence, we can rewrite the objective in terms of $\widehat{\mu}$ parametrized by $\widehat{\pi}$. The terms
    \begin{align*}
        H(\widehat{\mu}) + \sum_{x\in\mathcal{X}}\sum_{k=0}^\kappa \widehat{\mu}(x, k)\log{\kappa\choose k} = -(\kappa-1) H(\widehat{\mu}_0) + \kappa H(\pi_{\widehat{\mu}}).
    \end{align*}
    Thus, we can write the objective as
    \begin{equation*}
        \psi(\beta, \kappa) = \sup_{\widehat{\pi}\in\Delta} \tilde{V}(\beta, \kappa, \widehat{\pi}) = \sup_{\widehat{\pi}\in\Delta} \left[\mathbb{E}_{\widehat{\mu}_0} \beta X_0 + \frac\kappa2 H(\widehat{\pi}) - (\kappa-1) H(\widehat{\mu}_0) - \log|\mathcal{X}| \right]
    \end{equation*}

\end{proof}

\begin{lemma}
    \begin{enumerate}
        The unique optimizers of the Hardcore Ising optimization problem
        \begin{equation*}
            \psi(\beta, \kappa) = \sup_{\widehat{\pi}\in\Delta} \tilde{V}(\beta, \kappa, \widehat{\pi}) = \sup_{\widehat{\pi}\in\Delta} \left[\mathbb{E}_{\widehat{\mu}_0} \beta X_0 + \frac\kappa2 H(\widehat{\pi}) - (\kappa-1) H(\widehat{\mu}_0) - \log|\mathcal{X}| \right]
        \end{equation*}
        are in the form
        \begin{equation}
            \begin{aligned}
                \widehat{\pi}^{00}                      & = \displaystyle\frac{\bar{q}}{2+\bar{q}} \\
                \widehat{\pi}^{01} = \widehat{\pi}^{10} & = \displaystyle\frac{1}{2+\bar{q}}.
            \end{aligned}
            \label{Eq.HAIS-pi-qbar}
        \end{equation}
        with $\bar{q}$ being a solution to
        \begin{equation}
            \beta + \kappa \log \bar{q} - (\kappa-1)\log (1+\bar{q}) = 0.
            \label{Eq.HAIS-q-1}
        \end{equation}
    \end{enumerate}
\end{lemma}

\begin{proof}
    Parametrizing with (\ref{Eq.HAIS-pi-param}), the gradient is
    \begin{equation}
        \frac{d\widetilde{V}}{d\widehat{\pi}^{00}} = -\frac\beta2-\frac\kappa2 \left[\log\widehat{\pi}^{00} - \log \frac12(1 - \widehat{\pi}^{00})\right] - \frac{\kappa-1}{2}\log\frac{1-\widehat{\pi}^{00}}{1+\widehat{\pi}^{00}} = 0.
        \label{Eq.HAIS-pi-Lagrange}
    \end{equation}

    To solve this, we first write $\widehat{\pi}$ in terms of $q^{x, x'}$ as found in (\ref{Eq.HAIS-pimu-q}). Applying the normality constraint and symmetry constraints on $q^{x, x'}$, we have
    \begin{align}
        (q^{00} + q^{01})^\kappa + (q^{10})^\kappa & = 1               \label{Eq.HAIS-q-norm}  \\
        q^{01}(q^{00}+q^{01})^{\kappa-1}           & = (q^{10})^\kappa \label{Eq.HAIS-q-symm}.
    \end{align}
    Substituting (\ref{Eq.HAIS-q-symm}) into (\ref{Eq.HAIS-q-norm}), we have
    \begin{equation*}
        (q^{00}+q^{01})^{\kappa-1} = \frac{1}{2q^{01}+q^z{00}}.
    \end{equation*}
    Rewriting $\widehat{\pi}$:
    \begin{align*}
        \widehat{\pi}^{00}                      & = \frac{q^{00}}{2q^{01}+q^{00}}  \\
        \widehat{\pi}^{01} = \widehat{\pi}^{10} & = \frac{q^{10}}{2q^{01}+q^{00}}.
    \end{align*}
    Let $\bar{q} = \frac{q^{00}}{q^{01}}$, we have
    \begin{equation*}
        \begin{aligned}
            \widehat{\pi}^{00}                      & = \displaystyle\frac{\bar{q}}{2+\bar{q}} \\
            \widehat{\pi}^{01} = \widehat{\pi}^{10} & = \displaystyle\frac{1}{2+\bar{q}}.
        \end{aligned}
    \end{equation*}
    Substituting (\ref{Eq.HAIS-pi-qbar}) into (\ref{Eq.HAIS-pi-Lagrange}), we have
    \begin{equation*}
        \beta + \kappa \log \bar{q} - (\kappa-1)\log (1+\bar{q}) = 0.
    \end{equation*}
    or
    \begin{equation}
        \frac{1}{1+\bar{q}} = e^\beta \left(\frac{\bar{q}}{1+\bar{q}}\right)^\kappa
        \label{Eq.HAIS-q-2}
    \end{equation}

    To prove that this is unique, it is sufficient to show that $\tilde{V}(\beta, \kappa, \widehat{\pi})$ is concave in $\widehat{\pi}$.
    More specifically, since $\widehat{\pi}\in\Delta$ can be parametrized by some $\widehat{\pi}^{00}\in[0, 1]$,
    we only need to show that $\widetilde{V}(\beta, \kappa, \pi^{00})$ is concave.

    Indeed,
    \begin{equation*}
        \frac{d^2\widetilde{V}}{(d\widehat{\pi}^{00})^2} = \frac{\left(\frac\kappa2-1\right)(\widehat{\pi}^{00} - 1) - 1}{\widehat{\pi}^{00} - (\widehat{\pi}^{00})^3} < 0, \qquad \forall \widehat{\pi}^{00}\in[0, 1], \kappa \geq 2.
    \end{equation*}
    Therefore, the above stationary point is indeed is a unique maximizer of $\widetilde{V}$.

\end{proof}

\begin{lemma}
    Given $\beta$ and $\kappa$, there exists a unique $h$ that solves
    \begin{equation*}
        h = (\kappa - 1)\log (1+e^{-h}) - \beta
    \end{equation*}
\end{lemma}

\begin{proof}
    Solving for $h$ is equivalent to solving $f(h; \beta) = (\kappa - 1)\log (1+e^{-h}) - \beta - h = 0$.
    First we show uniqueness. Notice that $f(h; \beta)$ is monotonic in $h$:
    \begin{equation*}
        \frac{\partial f}{\partial h} = -\frac{(\kappa - 1)e^{-h}}{1+e^{-h}} - 1 > 0 \qquad \forall h\in \mathbb{R}
    \end{equation*}
    This shows uniqueness of a root of $f(\cdot; \beta)$.

    Next, we show that $f(h; \beta)$ can take both positive and negative values. Notice that when
    $h\rightarrow \infty$, $\log (1+e^{-h}) \rightarrow 0$, so $f(h; \beta) \rightarrow -\infty$. Thus, the function can take negative values.
    For positive values of $f(h; \beta)$, consider $h = -\beta$. We have $f(h; \beta) = (\kappa - 1)\log (1+e^\beta) > (\kappa - 1)\log(1) = 0$

    By the intermediate function theorem, since $f(h; \beta)$ is continuous in $h$, there exists a solution for all $\beta$.

\end{proof}

Based on the Lemma 7.3, we define
\begin{align*}
    h: \mathbb{R} \times \mathbb{Z}_{\geq 2} & \mapsto \mathbb{R}       \\
    (\beta, \kappa)                          & \mapsto h(\beta, \kappa)
\end{align*}
where $h(\beta, \kappa) = (\kappa - 1)\log (1+e^{-h(\beta, \kappa)}) - \beta$

\begin{lemma}
    $h(\beta, \kappa)$ has the following properties:
    \begin{enumerate}
        \item The function is monotonically decreasing with respect to $\beta$,
        \item The function is monotonically increasing with respect to $\kappa$,
        \item The function is convex with respect to $\beta$,
        \item $\lim_{\beta\rightarrow\infty} h(\beta, \kappa) \rightarrow -\infty$ and $\lim_{\beta\rightarrow-\infty} h(\beta, \kappa) \rightarrow \infty$,
        \item $\lim_{\kappa\rightarrow\infty} h(\beta, \kappa) \rightarrow \infty$
    \end{enumerate}
\end{lemma}

\begin{proof}
    \begin{enumerate}
        \item Implicit differentiation gives
              \begin{equation*}
                  \frac{\partial h}{\partial\beta} = -\frac{1+e^{-h}}{1+\kappa e^{-h}} < 0, \qquad \forall h\in \mathbb{R}.
              \end{equation*}
              Thus, $h$ monotonically decreases with $\beta$.
        \item Similarly,
              \begin{equation*}
                  \frac{\partial h}{\partial \kappa} = \log(1+e^-h)\frac{1+e^{-h}}{1+\kappa e^{-h}} > 0, \qquad \forall h\in \mathbb{R}, \kappa\in\mathbb{Z}_{\geq 2}
              \end{equation*}
        \item Again, we have
              \begin{align*}
                  \frac{\partial^2h}{\partial\beta^2} = \left(-\frac{1+e^{-h}}{1+\kappa e^{-h}}\right)\frac{\partial}{\partial h}\left(-\frac{1+e^{-h}}{1+\kappa e^{-h}}\right) = \frac{[(\kappa-1)e^{-h}]^2}{(1+\kappa e^{-h})^3} > 0 .
              \end{align*}
              Thus, $h(\beta,\kappa)$ is convex.
        \item The inverse of $h(\beta, \kappa)$ is $\beta = h-(\kappa-1)\log(1+e^{-h})$. This means that $h(\beta,\kappa)$ is bijective.
              A bijective, continuous, monotonically decreasing function has these limits.
        \item Similar to part 4. $\kappa = \frac{\beta+h}{\log(1+e^{-h})} + 1$
    \end{enumerate}
\end{proof}

\begin{theorem}
    The optimization problem characterizing the annealed pressure of the Hardcore Ising model on a marked $\kappa$-star
    \begin{equation*}
        \psi(\beta, \kappa) = \sup_{\mu\in M} \left\{\mathbb{E}_\mu\left[\beta X_0\right] - \left[H(\mu\|\eta) - \frac\kappa2 H(\pi_\mu\| \pi_\eta)\right]\right\}
    \end{equation*}
    has a unique optimizer
    \begin{equation}
        \mu(\mathbf{x}, \tau; \beta, \kappa) = \frac{1}{Z(\beta, \kappa)} \exp\left[\beta x_0 + h(\beta, \kappa) \sum_{v\in\mathcal{N}_o(\tau)}(1-x_v)\right],
    \end{equation}
    with $h(\beta, \kappa)$ being the unique solution to
    \begin{equation}
        h = (\kappa-1)\log(1+e^{-h}) - \beta,
        \label{Eq.HAIS-h}
    \end{equation}
    and $Z(\beta, \kappa) = (1+e^{h(\beta, \kappa)})^\kappa + e^{\beta + \kappa h(\beta, \kappa)}$.

    The corresponding annealed pressure is
    \begin{equation}
        \psi(\beta, \kappa) = \beta + h - \left(\frac\kappa2 - 1\right) \log(2e^{-h} + 1) - \log 2
    \end{equation}
\end{theorem}

\begin{proof}
    From Lemma 7.2, we have shown that the unique maximizer of $\widetilde{V}$ exists in following form:
    \begin{equation*}
        \begin{aligned}
            \widehat{\pi}^{00}                      & = \displaystyle\frac{\bar{q}}{2+\bar{q}} \\
            \widehat{\pi}^{01} = \widehat{\pi}^{10} & = \displaystyle\frac{1}{2+\bar{q}},
        \end{aligned}
    \end{equation*}
    with $\bar{q}$ being a solution to
    \begin{equation*}
        \beta + \kappa \log \bar{q} - (\kappa-1)\log (1+\bar{q}) = 0.
    \end{equation*}

    We can then calculate the root mark distributions:
    \begin{align}
        \mu_0(0) & = \frac{1+\bar{q}}{2+\bar{q}} = \frac{1}{1+\frac{1}{1+\bar{q}}}  \nonumber       \\
                 & = \frac{1}{1 + e^\beta \left(\frac{\bar{q}}{1+\bar{q}}\right)^\kappa} \nonumber  \\
                 & = \frac{(1+\bar{q})^\kappa }{(1+\bar{q})^\kappa + e^\beta (\bar{q})^\kappa}
        \intertext{Similarly, }
        \mu_0(1) & = \frac{e^\beta (\bar{q})^\kappa}{(1+\bar{q})^\kappa + e^\beta (\bar{q})^\kappa}
    \end{align}
    We then have distribution
    \begin{equation*}
        \begin{cases}
            \mu(0, k) = \displaystyle\frac{1}{(1+\bar{q})^\kappa + e^\beta (\bar{q})^\kappa}{\kappa\choose k} (\bar{q})^k \\
            \mu(1, 0) = \displaystyle\frac{e^\beta (\bar{q})^\kappa}{(1+\bar{q})^\kappa + e^\beta (\bar{q})^\kappa}
        \end{cases}
    \end{equation*}
    Alternatively, writing $\bar{q} = \exp h$,
    \begin{equation}
        \begin{cases}
            \mu(0, k) = \frac1Z{\kappa\choose k} e^{kh} \\
            \mu(1, 0) = \frac1Z e^{\beta + \kappa h}
        \end{cases}
    \end{equation}
    where $Z = (1+e^h)^\kappa + e^{\beta + \kappa h}$.

    In Lemma 7.3, we have shown that for given $\beta$ and $\kappa$, $h$ exists and is unique.
    This makes $\mu$ well-defined for all $\beta$ and $\kappa$.

    To see that $\mu$ is a unique maximizer, note that in Lemma 7.1, we have shown that for each edge distribution $\widehat{\pi}$, $\widehat{\mu}$ is the unique maximizer for $V$.
    In other words, the unique maximizer for $\widetilde{V}$ gives a unique maximizer for $V$. Since in Lemma 7.2, we have shown that $\widehat{\pi}$ is
    indeed a unique maximizer for $\widetilde{V}$, the above expression is indeed a unique maximizer.

    An alternative form of $\mu$ is
    \begin{equation}
        \mu(\mathbf{x}, \tau) = \frac1Z \exp\left[\beta x_0 + h \sum_{v\in\mathcal{N}_o(\tau)}(1-x_v)\right]
    \end{equation}
    where
    \begin{equation*}
        h = (\kappa - 1)\log (1+e^{-h}) - \beta
    \end{equation*}

    For the annealed pressure, recall from Lemma 7.1 that it is equivalent to
    \begin{equation*}
        \psi(\beta, \kappa) = \sup_{\widehat{\pi}} \left[\mathbb{E}_{\widehat{\mu}_0}(\beta X_0) - (\kappa-1)H(\widehat{\mu}_0) + \frac\kappa2H(\widehat{\pi}) - \log|\mathcal{X}|\right]
    \end{equation*}
    Substitute (\ref{Eq.HAIS-pi-qbar}) and $\bar{q}=e^h$, we have
    \begin{align*}
        \mathbb{E}_{\widehat{\mu}_0}(\beta X_0) & = \beta \frac{1}{2+e^h}                        \\
        H(\widehat{\mu}_0)                      & = \log(2+e^h) - \frac{1+e^h}{2+e^h}\log(1+e^h) \\
        H(\widehat{\pi})                        & = \log(2+e^h) - \frac{he^h}{2+e^h}
    \end{align*}
    Hence
    \begin{equation*}
        \psi(\beta, \kappa) = \frac{\beta - \frac\kappa2he^h}{2+e^h} - \left(\frac\kappa2 - 1\right)\log(2+e^h) + (\kappa-1)\frac{1+e^h}{2+e^h}\log(1+e^h) - \log 2
    \end{equation*}
    Recall (\ref{Eq.HAIS-q-1}). Substitute $\bar{q}=e^h$ and we have
    \begin{equation}
        \beta + \kappa h - (\kappa-1)\log(1+e^h) = 0
    \end{equation}
    so
    \begin{equation*}
        \psi(\beta, \kappa) = \frac{\beta - \frac\kappa2he^h}{2+e^h} - \left(\frac\kappa2 - 1\right)\log(2+e^h) + \frac{(\beta + \kappa h)(1+e^h)}{2+e^h}- \log 2
    \end{equation*}
    which simplifies to
    \begin{equation}
        \psi(\beta, \kappa) = \beta + \frac{\kappa h}{2} - \left(\frac\kappa2 - 1\right)\log(2 + e^h)- \log 2
        \label{Eq.HAIS-psi1}
    \end{equation}
    or
    \begin{equation}
        \psi(\beta, \kappa) = \beta + h - \left(\frac\kappa2 - 1\right)\log(2e^{-h} + 1)- \log 2
        \label{Eq.HAIS-psi2}
    \end{equation}
\end{proof}

\newpage

\subsection{Hardcore Ising Cross Reference}

We will now check the analytical results to the numerical results. Note that
\begin{equation*}
    h = \log\frac{1 - 2 \mu_0(1)}{\mu_0(1)}
\end{equation*}
From data gathered in Fig.\ref{Fig.HAIS-RM}, we see that the analytical results agree with numerical
\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{img/HA_Ising/h_value.png}
    \caption{Hardcore Ising, comparison of numerical and analytical predictions of $h$. The dashed lines show analytical predictions.}
    \label{Fig.HAIS-h-value}
\end{figure}

Regarding the edge distribution, recall that when $\beta\rightarrow\infty$, $h\rightarrow -\infty$, and when $\beta\rightarrow-\infty$, $h\rightarrow \infty$.

We write $\pi_\mu$ in terms of $h$:
\begin{equation}
    \begin{aligned}
        \pi_\mu(0, 0)                 & = \frac{1}{2e^{-h} + 1} \\
        \pi_\mu(1, 0) = \pi_\mu(0, 1) & = \frac{1}{2+e^h}
    \end{aligned}
    \label{Eq.HAIS-pimu-h}
\end{equation}
When $\beta\rightarrow\infty$, $h\rightarrow-\infty$, so $\pi_\mu(0, 0)\rightarrow 0$, and $\pi_\mu(1, 0) = \pi_\mu(0, 1)\rightarrow \frac12$.

When $\beta\rightarrow-\infty$, $h\rightarrow\infty$, so $\pi_\mu(0, 0)\rightarrow 1$, and $\pi_\mu(1, 0) = \pi_\mu(0, 1)\rightarrow 1$.

This agrees with our predictions in Fig.\ref{Fig.HAIS-RL-beta-trends}.

\newpage

Barbier et al. \footcite{barbier_hard-core_2013} gives the annealed pressure as the following form:
\begin{equation}
    \psi(\beta, \kappa) = -\log(1+\bar{\pi})+\frac\kappa2\log(1-\bar{\pi}^2)
\end{equation}
where
\begin{equation*}
    \bar{\pi} = e^\beta (1-\bar{\pi})^\kappa
\end{equation*}
Our form is similar but missing a sign, as shown below.

Let
\begin{equation}
    \bar{\pi} = \frac{1}{1+e^h}
\end{equation}
We have
\begin{equation*}
    h = \log\frac{1-\bar{\pi}}{\bar{\pi}}
\end{equation*}
Rewrite (\ref{Eq.HAIS-q-1}) as
\begin{equation}
    \log(\bar{\pi}) = \beta + \kappa \log(1 - \bar{\pi})
\end{equation}
or
\begin{equation}
    \bar{\pi} = e^\beta(1 - \bar{\pi})^\kappa
\end{equation}

From (\ref{Eq.HAIS-h}), we have
\begin{equation*}
    \beta + h = -(\kappa-1)\log(1-\bar{\bar{\pi}})
\end{equation*}
and
\begin{equation*}
    \log(2e^{-h}+1) = \log\frac{1+\bar{\pi}}{1-\bar{\pi}}
\end{equation*}
Substituting into (\ref{Eq.HAIS-psi2}), and simplifying, we have
\begin{equation}
    \psi(\beta, \kappa) = \log(1+\bar{\pi}) - \frac\kappa2\left(1-\bar{\pi}^2\right) - \log 2
\end{equation}

\newpage

\section{Jul 18 - Jul 21}

\subsection{Numerical Methods}
We consider independent probability measures $\mu^a$ and $\mu^v$, on $\ell$-star and $k$-star respectively.
Since the leaves are considered identical, we can write $\mu^a(x, j)$ and $\mu^v(x, i)$ to represent the probabilities
that the root mark is $x$ and there are $j$ and $i$ leaves with mark $+$ respectively.

The edge distribution is
\begin{align*}
    \begin{cases}
        \displaystyle\pi_{\mu^a} (x_0, +) = \frac1\ell\sum_{j=0}^\ell j \mu^a(x_0, j) \\
        \displaystyle\pi_{\mu^a} (x_0, -) = \frac1\ell\sum_{j=0}^\ell (\ell-j) \mu^a(x_0, j)
    \end{cases}
    \intertext{and}
    \begin{cases}
        \displaystyle\pi_{\mu^v} (x_0, +) = \frac1k\sum_{i=0}^k k \mu^v(x_0, i) \\
        \displaystyle\pi_{\mu^v} (x_0, -) = \frac1k\sum_{i=0}^k (k-i) \mu^v(x_0, i).
    \end{cases}
\end{align*}

The symmetry constraint gives
\begin{equation*}
    \pi_{\mu^a} (x_0, x_v) = \pi_{\mu^v} (x_v, x_0),\qquad \forall x_0, x_v\in\mathcal{X}.
\end{equation*}

We want to solve the optimization problem
\begin{align*}
    \psi(\beta, B, \ell, k) = & \sup_{\mu^a, \mu^v}\left\{\alpha\mathbb{E}_{\mu^a}\left[\frac\beta2\sum_v x_0 x_v + B x_0\right] + \mathbb{E}_{\mu^v}\left[\frac\beta2\sum_v x_0 x_v + B x_0\right] \right.      \\
                              & - \alpha\frac12\left[H(\mu^a\|\widecheck{\mu}^a) + H(\mu^a\|\widehat{\mu}^a)\right] \left.- \frac12\left[H(\mu^v\|\widecheck{\mu}^v) + H(\mu^v\|\widehat{\mu}^v)\right]\right\},
\end{align*}
where
\begin{equation*}
    \begin{cases}
        \displaystyle\widecheck{\mu}^a(x_0, j) = \frac12{\ell \choose j}[\pi_{\mu^a}^v(+)]^j[\pi_{\mu^a}^v(-)]^{\ell-j} \\
        \displaystyle\widecheck{\mu}^v(x_0, i) = \frac12{k \choose j}[\pi_{\mu^a}^v(+)]^i[\pi_{\mu^a}^v(-)]^{k-i}
    \end{cases}
\end{equation*}
and
\begin{equation*}
    \begin{cases}
        \displaystyle\widehat{\mu}^a(x_0, j) = \frac12{\ell \choose j}[\pi_{\mu^a}^{v|0}(+|x_0)]^j[\pi_{\mu^a}^v(-|x_0)]^{\ell-j} \\
        \displaystyle\widehat{\mu}^v(x_0, i) = \frac12{k \choose j}[\pi_{\mu^a}^{v|0}(+|x_0)]^i[\pi_{\mu^a}^{v|0}(-|x_0)]^{k-i},
    \end{cases}
\end{equation*}
with
\begin{align*}
    \pi_{\mu^{(\cdot)}}^0(x_0)         & = \sum_{x_v\in\mathcal{X}}\pi_{\mu^{(\cdot)}}(x_0, x_v)             \\
    \pi_{\mu^{(\cdot)}}^v(x_v)         & = \sum_{x_0\in\mathcal{X}}\pi_{\mu^{(\cdot)}}(x_0, x_v)             \\
    \pi_{\mu^{(\cdot)}}^{v|0}(x_v|x_0) & = \frac{\pi_{\mu^{(\cdot)}}(x_0, x_v)}{\pi_{\mu^{(\cdot)}}^0(x_0)}.
\end{align*}

The SLSQP optimizer in SciPy will be used. We initialize with $\text{Uniform}(-1, 1)$ for all entries in $\mu^a$ and $\mu^v$.
The objective tolerance is $10^{-10}$, and the tolerance for normality and symmetry constraints is $10^{-15}$ (We must allow for some
wiggle room to find the optimizer). We take 5 such trials with different initial values and choose the one that maximizes the objective.
For each trial, we retry if a failure is encountered.

\subsection{Conditional Independence Numerical}

We hypothesize that two distinct leaves chosen uniformly from the $a$ or the $v$ trees are conditionally independent on the root mark, i.e.
\begin{align*}
    \mathbb{P}_{\mu^{(\cdot)}}(X_v=x_v, X_{v'}=x_{v'}|X_0=x_0) = \mathbb{P}_{\mu^{(\cdot)}}(X_v=x_v|X_0=x_0)\mathbb{P}_{\mu^{(\cdot)}}(X_{v'}=x_{v'}|X_0=x_0)
\end{align*}
In other words, we hypothesize that
\begin{equation*}
    \begin{cases}
        \mu^a(x_0, j)=\pi_{\mu^a}^0(x){\ell\choose j}[\pi_{\mu^a}^{v|0}(+|x_0)]^j[\pi_{\mu^a}^{v|0}(-|x_0)]^{\ell-j} \\
        \mu^v(x_0, i)=\pi_{\mu^v}^0(x){k\choose i}[\pi_{\mu^v}^{v|0}(+|x_0)]^i[\pi_{\mu^v}^{v|0}(-|x_0)]^{k-i}
    \end{cases}
\end{equation*}
Since by the symmetry constraint, $\pi_{\mu^a}(x_0, x_v)=\pi_{\mu^a}(x_v, x_0)$, $\pi_{\mu^a}$ and $\pi_{\mu^v}$ are transposed versions
of each other. This means that the optimizer can be parametrized by some $\pi_\mu$.

Indeed, the leaf mark distributions, given the root mark, are independent. We can see this by looking at the mutual information
between two distinct leaves chosen uniformly.

\begin{figure}[h]
    \centering
    \includegraphics[width=16cm]{img/Ising_bptt/CI_l3_k3.png}
    \caption{For Ising on a bipartite graph, $l=3$, $k=3$, leaves are conditionally independent given the root mark. It is true for both the $a$ star
        and the $k$ star.}
    \label{Eq.ISBP-CI-l3-k3}
\end{figure}

Notice that sometimes we get spikes in mutual information, but they are not
consistent and disappear after a trial with a different seed, so they are likely failed optimization attempts.
Among the 900 or so optimization attempts, about 10 will fail.

\begin{figure}[!h]
    \centering
    \includegraphics[width=16cm]{img/Ising_bptt/CI_l4_k4.png}
    \includegraphics[width=16cm]{img/Ising_bptt/CI_l4_k5.png}
    \includegraphics[width=16cm]{img/Ising_bptt/CI_l5_k5.png}
    \includegraphics[width=16cm]{img/Ising_bptt/CI_l5_k6.png}
    \includegraphics[width=16cm]{img/Ising_bptt/CI_l6_k6.png}
    \caption{For Ising on a bipartite graph, leaves are conditionally independent given the root mark. We attempted on higher $l$ and $k$ values, and it seems to hold.}
    \label{Eq.ISBP-CI-large-lk}
\end{figure}

\newpage

\subsection{Conditional Independence Analytical}

In this part, we will try to prove conditional independence. First, we notice that we can split the objective function into two independent parts.
Since $\mu^a$ and $\mu^v$ are independent,
\begin{equation}
    \begin{aligned}
        V(\beta, B, \ell, k, \mu^a, \mu^v) = \alpha & \left\{\mathbb{E}_{\mu^a}\left[\frac\beta2\sum_{v=1}^l x_0x_v + B x_0\right]
        - \frac12\left[H(\mu^a\|\widecheck{\mu}^a)+H(\mu^a\|\widehat{\mu}^a)\right]\right\} +                                      \\
                                                    & \left\{\mathbb{E}_{\mu^v}\left[\frac\beta2\sum_{v=1}^k x_0x_v + B x_0\right]
        - \frac12\left[H(\mu^v\|\widecheck{\mu}^v)+H(\mu^v\|\widehat{\mu}^v)\right]\right\},
    \end{aligned}
\end{equation}
Can be solved quite easily by fixing one distribution.
Say we fix $\mu^v$. By the symmetry constraint,
\begin{equation*}
    \pi_{\mu^a}(x_0, x_v) = \pi_{\mu^a}(x_v, x_0),
\end{equation*}
which means that the edge distribution is fixed. Therefore, the optimization problem becomes
\begin{equation}
    \sup_{\substack{\mu^a\in M \\ \pi_{\mu^a}=\pi}} \left\{\mathbb{E}_{\mu^a}\left[\frac\beta2\sum_{v=1}^l x_0x_v + B x_0\right]
    - \frac12\left[H(\mu^a\|\widecheck{\mu}^a)+H(\mu^a\|\widehat{\mu}^a)\right]\right\}
\end{equation}
for $\pi$ a probability measure on $\mathcal{X}^2$. In this case, $\pi(x_0, x_v) = \pi_{\mu^v}(x_v, x_0)$. We define the new objective as $U^a(\beta, B, \ell, \mu^a; \pi)$.
This is a similar problem to the one we solved for Ising on regular trees. We have
\begin{align*}
    \mathbb{E}_{\mu^a}\left[\frac\beta2\sum_{v=1}^l x_0x_v + B x_0\right] & = \mathbb{E}_\pi\left[\frac{\beta\ell}{2}x_0 x_v+Bx_0\right]                                                                        \\
    H(\mu^a\|\widecheck{\mu}^a)                                           & = -H(\mu^a)-\sum_{x\in\mathcal{X}}\sum_{j=0}^\ell\mu^a(x, j)\log{\ell\choose j} +\ell H(\pi_{\mu^a}^1)  + \log2                     \\
    H(\mu^a\|\widehat{\mu}^a)                                             & = -H(\mu^a)-\sum_{x\in\mathcal{X}}\sum_{j=0}^\ell\mu^a(x, j)\log{\ell\choose j} +\ell H(\pi_{\mu^a})-\ell H(\pi_{\mu^a}^0) + \log2.
\end{align*}
Thus, neglecting constants, write the objective as
\begin{equation*}
    \mathbb{E}_{\pi_{\mu^a}}\left[\frac{\beta\ell}{2}x_0 x_v+Bx_0\right] + H(\mu^a) + \sum_{x\in\mathcal{X}}\sum_{j=0}^\ell\mu^a(x, j)\log{\ell\choose j} - \frac\ell2\left[H(\pi_{\mu^a}^1)+H(\pi_{\mu^a})-H(\pi_{\mu^a}^0)\right].
\end{equation*}
Taking $\pi_{\mu^a}=\pi$, most terms become constants, and we are left with
\begin{equation*}
    H(\mu^a) + \sum_{x\in\mathcal{X}}\sum_{j=0}^\ell\mu^a(x, j)\log{\ell\choose j}.
\end{equation*}
We can solve this problem with Lagrange multipliers, constraints being $\pi_{\mu^a}=\pi$. Solving for the constraint, we get optimizer
\begin{equation}
    \bar{\mu}^a(x_0, j; \pi) = \pi^0(x_0){\ell\choose j}\left[\frac{\pi(x_0,x_0)}{\pi^0(x_0)}\right]^j\left[\frac{\pi(x_0,-x_0)}{\pi^0(x_0)}\right]^{\ell-j}.
\end{equation}
Similarly, for $\mu^v$, we can set $\pi_{\mu^v}(x_0, x_v) = \pi(x_v, x_0)$, the transpose of $\pi$, and obtain
\begin{equation}
    \bar{\mu}^v(x_0, i; \pi) = \pi^1(x_0){k\choose i}\left[\frac{\pi(x_0,x_0)}{\pi^1(x_0)}\right]^i\left[\frac{\pi(-x_0,x_0)}{\pi^1(x_0)}\right]^{k-i}.
\end{equation}

Plug in the expressions, we have
\begin{equation*}
    H(\bar{\mu}^a(\cdot; \pi)) + \sum_{x\in\mathcal{X}}\sum_{j=0}^\ell\bar{\mu}^a(\cdot; \pi)\log{\ell\choose j} = H(\pi^0)+ \ell H(\pi) - \ell H(\pi^0)
\end{equation*}
and
\begin{equation}
    U^a(\beta, B, \ell, \bar{\mu}^a(\cdot; \pi); \pi) = \mathbb{E}_{\pi}\left[\frac{\beta\ell}{2}x_0 x_v+Bx_0\right]+H(\pi^0)-\frac\ell2\left[H(\pi^0)+H(\pi^1)-H(\pi)\right].
\end{equation}
Similarly,
\begin{equation}
    U^v(\beta, B, k, \bar{\mu}^v(\cdot; \pi); \pi) = \mathbb{E}_{\pi}\left[\frac{\beta k}{2}x_0 x_v+Bx_v\right]+H(\pi^1)-\frac{k}{2}\left[H(\pi^0)+H(\pi^1)-H(\pi)\right].
\end{equation}
Hence, rewriting the objective, define
\begin{equation}
    \widetilde{V}(\beta, B, \ell, k, \pi) = \mathbb{E}_{\pi}\left[\beta k x_0 x_v + \alpha Bx_0 + B x_v\right] - (k-\alpha)H(\pi^0) - (k-1)H(\pi^1) + kH(\pi)
\end{equation}

\subsection{Solve for edge distribution}

Taking the Lagrange multiplier of the above target $\widetilde{V}(\beta, B, \ell, k, \pi)$ on $\pi$, we have
\begin{equation*}
    \frac{\partial \widetilde{V}}{\partial \pi(x, x')} = \beta k x x' + \alpha B x + B x' + (k+\alpha) \log \pi^0(x) + (k+1)\log\pi^1(x') - k\log\pi(x, x') + \lambda = 0,
\end{equation*}
where $\lambda$ is a constant coming from the Lagrange multiplier of $\sum_{x, x'\in\mathcal{X}} \pi(x, x') = 1$.

Since $\pi$ is a probability measure on $\mathcal{X}^2$, we know that $\pi^0$ and $\pi^1$ are probability measures on $\mathcal{X}$.
Therefore, we can parametrize $\pi^0$ and $\pi^1$ as
\begin{equation}
    \begin{aligned}
        \pi^0(x)  & = \frac{e^{h^a x}}{e^{h^a} + e^{-h^a}}   \\
        \pi^1(x') & = \frac{e^{h^v x'}}{e^{h^v} + e^{-h^v}},
    \end{aligned}
    \label{Eq.ISBP-h-def}
\end{equation}
where $h^a, h^v\in\mathbb{R}$.

Substituting (\ref{Eq.ISBP-h-def}), rewrite
\begin{equation}
    \pi(x, x') = \frac1{Z^\pi} \exp\left[\beta xx' + \frac1\ell Bx + \frac1kBx' + \left(1-\frac1\ell\right)h^ax + \left(1-\frac1k\right)h^vx'\right],
\end{equation}
where the normalizing constant $Z^\pi=e^{\lambda/k}(2\cosh h^a)^{1/k-1}(2\cosh h^v)^{1/\ell-1}$.

Let
\begin{equation}
    \begin{cases}
        \bar{h}^a & = \frac1\ell B + \left(1 - \frac1k\right)h^a  \\
        \bar{h}^v & = \frac1k B + \left(1 - \frac1\ell\right)h^v,
    \end{cases}
    \label{Eq.ISBP-hbar-def}
\end{equation}
and $\pi$ becomes
\begin{equation}
    \pi(x, x') = \frac1{Z^\pi} \exp\left[\beta xx' + \bar{h}^a x + \bar{h}^v x'\right].
    \label{Eq.ISBP-pi-hbar}
\end{equation}

To solve for $\bar{h}^a$ and $\bar{h}^v$, we consider
\begin{equation*}
    e^{2h^a} = \frac{\pi^0(+)}{\pi^0(-)} = \exp(2\bar{h}^a) \frac{\cosh\left[\beta+\bar{h}^v\right]}{\cosh\left[-\beta+\bar{h}^v\right]}
\end{equation*}
By definition (\ref{Eq.ISBP-hbar-def}),
\begin{equation*}
    \begin{cases}
        h^a & = \displaystyle\frac{k(\ell\bar{h}^a - B)}{k\ell - \ell} \\
        h^v & = \displaystyle\frac{\ell(k\bar{h}^v - B)}{k\ell - k}.
    \end{cases}
\end{equation*}
Taking log and simplifying, get
\begin{equation}
    \bar{h}^a = B + (\ell - 1)\tanh^{-1}[\tanh(\beta)\tanh(\bar{h}^v)].
    \label{Eq.ISBP-hbar-a}
\end{equation}
Similarly, on $\pi^1$, we have
\begin{equation}
    \bar{h}^v = B + (k - 1)\tanh^{-1}[\tanh(\beta)\tanh(\bar{h}^a)]
    \label{Eq.ISBP-hbar-v}
\end{equation}

\subsection{Cross-checking with Numerical Results}

We can confirm this with numerics. We already showed that both $\mu^a$ and $\mu^v$ are parametrized by a probability $\pi$, so investigating the
$\pi$ distribution should be sufficient. First, we want to confirm that $\pi$ comes in this form:
\begin{equation}
    \pi(x, x') = \frac1{Z_\pi} \exp\left[\beta xx' + \bar{h}^a x + \bar{h}^v x'\right].
\end{equation}
To check this, we can try and recover $\bar{h}^v$ and $\bar{h}^a$ values from $\pi_{\mu^a}$, namely
\begin{equation}
    \begin{aligned}
        \frac12\log\frac{\pi(x, x)}{\pi(x, -x)}     & = \beta + \bar{h}^v x   \\
        \frac12\log\frac{\pi(x', x')}{\pi(-x', x')} & = \beta + \bar{h}^a x'.
    \end{aligned}
    \label{Eq.ISBP-h-num}
\end{equation}
Notice that by doing this, we can recover $\bar{h}^v$ values from both $x=+$ and $x=-$. If our form is correct,
the two predictions should equal each other for all $\beta$ and $B$. More specifically, for
\begin{equation*}
    \begin{cases}
        \bar{h}^v_+ & = -\frac12\log\frac{\pi(+, +)}{\pi(+, -)} + \beta \\
        \bar{h}^v_- & = \frac12\log\frac{\pi(-, -)}{\pi(-, +)} - \beta
    \end{cases}
\end{equation*}
we want $\bar{h}^v_- = \bar{h}^v_+$ for all $\pi$. We can plot $\bar{h}^v_+$ on the horizontal axis and $\bar{h}^v_-$
on the vertical axis for each pair of $\beta$ and $B$ values. They should all lie on the line with slope 1 through the
origin $y = x$. This is indeed true, with some noise due to optimizer not converging properly.
\begin{figure}[!h]
    \centering
    \includegraphics[width=16cm]{img/Ising_bptt/hv_pred.png}
    \caption{In the Ising bipartite model, we observe the edge distribution $\pi_{\mu^a}$. In (\ref{Eq.ISBP-h-num}), $\bar{h}^v$
        values recovered by taking different values of this distribution are the same. This is confirmed with some
        deviation (Note: these are not all the values.)}
    \label{Fig.ISBP-hv-guess}
\end{figure}

Similarly for $\bar{h}^a$, we compare
\begin{equation*}
    \begin{cases}
        \bar{h}^a_+ & = -\frac12\log\frac{\pi(+, +)}{\pi(-, +)} + \beta \\
        \bar{h}^a_- & = \frac12\log\frac{\pi(-, -)}{\pi(+, -)} - \beta
    \end{cases}
\end{equation*}
Indeed, they match our predictions.
\begin{figure}[!h]
    \centering
    \includegraphics[width=16cm]{img/Ising_bptt/ha_pred.png}
    \caption{In the Ising bipartite model, we observe the edge distribution $\pi_{\mu^v}$. In (\ref{Eq.ISBP-h-num}), $\bar{h}^v$
        values recovered by taking different values of this distribution are the same. This is confirmed with some
        deviation (Note: these are not all the values.)}
    \label{Fig.ISBP-ha-guess}
\end{figure}

\newpage

We can also check the cavity equation (\ref{Eq.ISBP-hbar-a}) and (\ref{Eq.ISBP-hbar-v}):
\begin{equation*}
    \begin{cases}
        \bar{h}^a & = B + (\ell - 1)\tanh^{-1}[\tanh(\beta)\tanh(\bar{h}^v)] \\
        \bar{h}^v & = B + (k - 1)\tanh^{-1}[\tanh(\beta)\tanh(\bar{h}^a)]
    \end{cases}
\end{equation*}

This is a bit complicated. There seems to be multiple solutions to the cavity equation. We will deal with it later.

\newpage

\subsection{Form of optimizer given cavity field}

To solve for $\mu^a$ and $\mu^v$, we first consider the root mark distributions:
\begin{equation*}
    \pi^0(x) = \frac1{Z_\pi}e^{\bar{h}^a x} 2\cosh(\beta x + \bar{h}^v)
\end{equation*}
we can see that
\begin{equation*}
    Z^\pi = 2e^{\bar{h}^a}\cosh(\beta + \bar{h}^v) + 2e^{-\bar{h}^a}\cosh(\beta - \bar{h}^v)
\end{equation*}
Thus
\begin{align*}
    \pi^0(x) & = \frac{e^{\bar{h}^a x} \cosh(\beta x + \bar{h}^v)}{e^{\bar{h}^a x}\cosh(\beta x + \bar{h}^v) + e^{-\bar{h}^ax}\cosh(\beta x - \bar{h}^v)} \\
             & = \frac{1}{1 + e^{-2\bar{h}^ax}\frac{\cosh(\beta x - \bar{h}^v)}{\cosh(\beta x + \bar{h}^v)}}
\end{align*}
We can rewrite (\ref{Eq.ISBP-hbar-a}) as
\begin{equation*}
    2\bar{h}^a x = 2 B x + (\ell - 1) \log \frac{\cosh(\beta x + \bar{h}^v)}{\cosh(\beta x - \bar{h}^v)},
\end{equation*}
for all $x\in\{-1, 1\}$.

Exponentiating and rearranging, we have
\begin{equation*}
    e^{2\bar{h}^ax}\left[\frac{\cosh(\beta x + \bar{h}^v)}{\cosh(\beta x - \bar{h}^v)}\right] = e^{2Bx}\left[\frac{\cosh(\beta x + \bar{h}^v)}{\cosh(\beta x - \bar{h}^v)}\right]^\ell.
\end{equation*}
Thus,
\begin{align*}
    \pi^0(x) & = \frac{1}{1 + e^{-2Bx}\left[\frac{\cosh(\beta x - \bar{h}^v)}{\cosh(\beta x + \bar{h}^v)}\right]^\ell}                         \\
             & = \frac{e^{Bx}[\cosh(\beta x + \bar{h}^v)]^\ell}{e^{B}[\cosh(\beta + \bar{h}^v)]^\ell + e^{-B}[\cosh(\beta - \bar{h}^v)]^\ell}.
\end{align*}
Therefore,
\begin{align*}
    \bar{\mu}^a(x, j; \pi) & = \pi^0(x){\ell \choose j} \left[\frac{\pi(x, x)}{\pi^0(x)}\right]^j \left[\frac{\pi(x, -x)}{\pi^0(x)}\right]^{\ell-j}      \\
                           & = \frac{e^{Bx}}{e^{B}[\cosh(\beta + \bar{h}^v)]^\ell + e^{-B}[\cosh(\beta - \bar{h}^v)]^\ell}
    {\ell \choose j} \left[e^{\beta xx + \bar{h}^a x + \bar{h}^v x}\right]^j
    \left[e^{\beta x(-x) + \bar{h}^a x + \bar{h}^v (-x)}\right]^{\ell - j}
    \intertext{Let $Z^a = e^{B}[\cosh(\beta + \bar{h}^v)]^\ell + e^{-B}[\cosh(\beta - \bar{h}^v)]^\ell$}
                           & = \frac1{Z^a} {\ell \choose j} \exp\left\{\beta x [jx - (\ell-j)x] + Bx + \bar{h}^a x + \bar{h}^v [jx - (\ell-j) x]\right\}
\end{align*}
Similarly, by calculating $\pi^1$,
\begin{equation*}
    \bar{\mu}^v(x, i; \pi) = \frac1{Z^v} {k \choose i} \exp\left\{\beta x [ix - (k-i)x] + Bx + \bar{h}^a [ix - (k-i) x] + \bar{h}^v x\right\}.
\end{equation*}
where
\begin{equation*}
    Z^v = e^{B}[\cosh(\beta + \bar{h}^a)]^k + e^{-B}[\cosh(\beta - \bar{h}^a)]^k.
\end{equation*}
The optimizers are in the form
\begin{equation}
    \begin{cases}
        \bar{\mu}^a(\mathbf{x}; \bar{h}^a, \bar{h}^v) = \displaystyle\frac1{Z^a} \exp\left(\beta \sum_{v=1}^\ell x_0x_v + Bx_0 + \bar{h}^a x_0 + \bar{h}^v \sum_{v=0}^\ell x_v\right) \\
        \bar{\mu}^v(\mathbf{x}; \bar{h}^a, \bar{h}^v) = \displaystyle\frac1{Z^v} \exp\left(\beta \sum_{v=1}^k x_0x_v + Bx_0 + \bar{h}^v x_0 + \bar{h}^a \sum_{v=0}^k x_v\right),
    \end{cases}
    \label{Eq.ISBP-soln}
\end{equation}
where
\begin{equation}
    \begin{cases}
        Z^a = e^{B}[\cosh(\beta + \bar{h}^v)]^\ell + e^{-B}[\cosh(\beta - \bar{h}^v)]^\ell \\
        Z^v = e^{B}[\cosh(\beta + \bar{h}^a)]^k + e^{-B}[\cosh(\beta - \bar{h}^a)]^k,
    \end{cases}
\end{equation}
and
\begin{equation*}
    \begin{cases}
        \bar{h}^a = B + (\ell - 1)\tanh^{-1}[\tanh(\beta)\tanh(\bar{h}^v)] \\
        \bar{h}^v = B + (k - 1)\tanh^{-1}[\tanh(\beta)\tanh(\bar{h}^a)].
    \end{cases}
\end{equation*}
Since $Z^\pi$ is represented differently in $\pi^0$ and $\pi^1$, generally, we have $Z^a \neq Z^v$. However, they are indeed related.
We can rewrite $Z^\pi$ in terms of $Z^a$ or $Z^v$, following the same steps as when we derived $\pi^0$ and $\pi^1$:
\begin{align*}
    Z^\pi & = 2e^{\bar{h}^a} \cosh(\beta + \bar{h}^v)\left[1 + e^{-2\bar{h}^a}\frac{\cosh(\beta - \bar{h}^v)}{\cosh(\beta + \bar{h}^v)}\right]             \\
          & = 2e^{\bar{h}^a} \cosh(\beta + \bar{h}^v)\left\{1 + e^{-2B}\left[\frac{\cosh(\beta - \bar{h}^v)}{\cosh(\beta + \bar{h}^v)}\right]^\ell\right\} \\
          & = 2e^{\bar{h}^a - B}[\cosh(\beta + \bar{h}^v)]^{-(\ell - 1)} Z^a
\end{align*}
Hence,
\begin{equation}
    \log Z^\pi = \log Z^a + \log 2 + (\bar{h}^a - B) - (\ell - 1) \log \cosh(\beta + \bar{h}^v).
    \label{Eq.ISBP-Zpi-Za}
\end{equation}
Similarly,
\begin{equation}
    \log Z^\pi = \log Z^v + \log 2 + (\bar{h}^v - B) - (k - 1) \log \cosh(\beta + \bar{h}^a).
    \label{Eq.ISBP-Zpi-Zv}
\end{equation}
Thus,
\begin{equation}
    \frac{Z^a}{Z^v} = \frac{e^{-\bar{h}^a}[\cosh(\beta + \bar{h}^v)]^{\ell - 1}}{e^{-\bar{h}^v}[\cosh(\beta + \bar{h}^a)]^{k - 1}}.
\end{equation}

\subsection{Bipartite Ising Annealed Pressure}

Up to a constant, the annealed pressure is equal to the following property:
\begin{equation*}
    \widetilde{V}(\beta, B, \ell, k, \pi) = \mathbb{E}_{\pi}\left[\beta k x_0 x_v + \alpha Bx_0 + B x_v\right] - (k-\alpha)H(\pi^0) - (k-1)H(\pi^1) + kH(\pi)
\end{equation*}
and we take
\begin{equation*}
    \pi(x, x') = \frac1{Z_\pi} \exp\left[\beta xx' + \bar{h}^a x + \bar{h}^v x'\right].
\end{equation*}
for $Z^\pi = 2e^{\bar{h}^a}\cosh(\beta + \bar{h}^v) + 2e^{-\bar{h}^a}\cosh(\beta - \bar{h}^v) = 2e^{\bar{h}^v}\cosh(\beta + \bar{h}^a) + 2e^{-\bar{h}^v}\cosh(\beta - \bar{h}^a)$,
and
\begin{equation*}
    \begin{aligned}
        \pi^0(x) = \frac1{Z^a}e^{Bx}[\cosh(\beta x + \bar{h}^v)]^\ell \\
        \pi^1(x) = \frac1{Z^v}e^{Bx}[\cosh(\beta x + \bar{h}^a)]^k.
    \end{aligned}
\end{equation*}
Notice that all of these distributions are exponential distributions, and for exponential distribution
\begin{equation*}
    p(\mathbf{x}) = \frac1Z \exp[T(\mathbf{x})],
\end{equation*}
where normalizing constant $Z = \int_{x\in\mathcal{X}} \exp[T(\mathbf{x})] \mu(d\mathbf{x})$,
we can write
\begin{equation*}
    H(p) = \log Z - \mathbb{E}_p [T(\mathbf{x})].
\end{equation*}
Thus,
\begin{align*}
    H(\pi)   & = \log Z^\pi - \mathbb{E}_\pi (\beta x_0 x_v + \bar{h}^a x_0 + \bar{h}^v x_v)    \\
    H(\pi^0) & = \log Z^a - \mathbb{E}_{\pi^0} (Bx_0 + \ell \log \cosh (\beta x_0 + \bar{h}^v)) \\
    H(\pi^0) & = \log Z^v - \mathbb{E}_{\pi^1} (Bx_v + k \log \cosh (\beta x_v + \bar{h}^a)).
\end{align*}
Plugging in, we have
\begin{align*}
    \widetilde{V}(\beta, B, \ell, k, \pi) = & k \log Z^\pi - (k-\alpha) \log Z^a - (k-1)\log Z^v                                  \\
                                            & + k\mathbb{E}_{\pi^0} (B - \bar{h}^a) x_0 + k\mathbb{E}_{\pi^1} (B - \bar{h}^v) x_v \\
                                            & + k (\ell - 1) \mathbb{E}_{\pi^0} \log\cosh(\beta x_0 + \bar{h}^v)
    + k (k - 1) \mathbb{E}_{\pi^1} \log\cosh(\beta x_v + \bar{h}^a)
\end{align*}
To rewrite $\mathbb{E}_{\pi^0} \log\cosh(\beta x_0 + \bar{h}^v)$ and $\mathbb{E}_{\pi^1} \log\cosh(\beta x_v + \bar{h}^a)$,
notice that we have (\ref{Eq.ISBP-Zpi-Za}), which gives
\begin{equation}
    \log Z^\pi = \log Z^a + \log 2 + (\bar{h}^a - B)x - (\ell - 1) \log \cosh(\beta x + \bar{h}^v),
\end{equation}
for all $x\in\{-1, 1\}$, since by rearranging the cavity equation (\ref{Eq.ISBP-hbar-v}), we have
\begin{equation*}
    (\bar{h}^a - B) - (\ell - 1) \log \cosh(\beta + \bar{h}^v) = -(\bar{h}^a - B) - (\ell - 1) \log \cosh(-\beta + \bar{h}^v).
\end{equation*}
This means we can rewrite the expectation
\begin{align*}
    \mathbb{E}_{\pi^0} (\ell - 1) \log\cosh(\beta x_0 + \bar{h}^v) & = \log Z^a - \log Z^\pi + \log 2 - \mathbb{E}_{\pi^0} (B - \bar{h}^a)x_0.
\end{align*}
Similarly,
\begin{align*}
    \mathbb{E}_{\pi^1} (k - 1) \log\cosh(\beta x_v + \bar{h}^a) & = \log Z^v - \log Z^\pi + \log 2 - \mathbb{E}_{\pi^1} (B - \bar{h}^v)x_v.
\end{align*}
Substituting in, we have an optimum
\begin{equation}
    \widetilde{V}(\beta, B, \ell, k, \pi) = \alpha \log Z^a + \log Z^v - k \log Z^\pi + 2k\log 2.
    \label{Eq.ISBP-AP-Z-form}
\end{equation}

\subsection{Cavity Equation Numerical}

We will check if solutions indeed satisfy the cavity equations. To do this, we recover the $\bar{h}^a$ and $\bar{h}^v$ values from
the numerical results, and check whether they satisfy the cavity equation
\begin{equation*}
    \begin{cases}
        B + (\ell - 1)\tanh^{-1}[\tanh(\beta)\tanh(\bar{h}^v)] - \bar{h}^a = 0 \\
        B + (k - 1)\tanh^{-1}[\tanh(\beta)\tanh(\bar{h}^a)] - \bar{h}^v = 0.
    \end{cases}
\end{equation*}
To do this, we plot the residue of this equation with respect to $\beta$ and $B$.
\begin{figure}[!h]
    \centering
    \includegraphics[width=12cm]{img/Ising_bptt/cavity_equation/l=3,k=3,B=-1.0.png}
    \includegraphics[width=12cm]{img/Ising_bptt/cavity_equation/l=3,k=3,B=-0.1.png}
    \includegraphics[width=12cm]{img/Ising_bptt/cavity_equation/l=3,k=3,B=-0.01.png}
    \caption{In Bipartite Ising model with $l=3$ and $k=3$, we perform 15 numerical optimizations with each pair of $\beta$ and $B$ values.
        $\bar{h}^a$ and $\bar{h}^v$ values recovered mostly satisfy the residue equations, with absolute residues mostly below $10^{-3}$.}
    \label{Fig.ISBP-CE-l3k3-1}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=12cm]{img/Ising_bptt/cavity_equation/l=3,k=3,B=0.0.png}
    \includegraphics[width=12cm]{img/Ising_bptt/cavity_equation/l=3,k=3,B=0.01.png}
    \includegraphics[width=12cm]{img/Ising_bptt/cavity_equation/l=3,k=3,B=0.1.png}
    \includegraphics[width=12cm]{img/Ising_bptt/cavity_equation/l=3,k=3,B=1.0.png}
    \caption{In Bipartite Ising model with $l=3$ and $k=3$, we perform 15 numerical optimizations with each pair of $\beta$ and $B$ values.
        $\bar{h}^a$ and $\bar{h}^v$ values recovered mostly satisfy the residue equations, with absolute residues mostly below $10^{-3}$.}
    \label{Fig.ISBP-CE-l3k3-2}
\end{figure}

\newpage

We can see that the residues are all fairly close to $0$, but for a few experiments, they clearly do not satisfy the equation, with residue
on the same order of magnitude as $\bar{h}^a$ and $\bar{h}^v$. We want to see if they result from the optimizer failing to converge
or if our analytical solution missed solutions. To do this, we see whether the ones with high residue satisfy the optimizer constraints,
and whether they have a higher objective value.

We find that distributions with high residues in the cavity equations has similar distributions in terms of constraint satisfaction.
\begin{figure}[h]
    \centering
    \includegraphics[width=15cm]{img/Ising_bptt/NC_l=3_k=3.png}
    \includegraphics[width=15cm]{img/Ising_bptt/SC_l=3_k=3.png}
    \caption{For each pair of $\beta$ and $B$ values, we calculate $11$ optimizers numerically.
        We separate them into two groups, one for where $\bar{h}^a$ and $\bar{h}^v$ residues are both smaller than $10^{-3}$,
        the other for ones that does not satisfy this condition. This shows the deviation from the distribution being normalized, or
        $\sum_{x, j} \mu(x, j) - 1$ deviation from symmetry constraint, or $\pi_{\mu^a}(x, x') - \pi_{\mu^v}(x', x)$.
        Ones with higher $\bar{h}$ residues do not have particularly high deviation in norm residue.
        The deviations are also small, with most bounded between $\pm 10^{-10}$.}
    \label{Fig.ISBP-NC-l3k3}
\end{figure}

This means that constraint satisfaction alone cannot be the reason that $\bar{h}$ is not satisfying the cavity equation.
We now analyze the objective function values.
\begin{figure}[!h]
    \centering
    \includegraphics[width=18cm]{img/Ising_bptt/OBJ_l=3_k=3.png}
    \caption{For each pair of $\beta$ and $B$ values, we calculate $11$ optimizers numerically. They are shown with crosses.
        Their color represent their absolute cavity equation residue on $\bar{h}^a$. We also calculate the $\bar{h}^a$ and
        $\bar{h}^v$ values, which, gives an analytical solution. We take the maximum of that. This is plotted as a line for comparison.}
    \label{Fig.ISBP-OBJ-l3k3}
\end{figure}

\newpage

First, notice that optimizers with higher cavity equation residue tend to stray further from the analytical optimal line.
Most of them deviate downwards, but a few of them are on the optimal line, and some even have higher objective function values.
While deviating downwards makes sense, as not satisfying the cavity equation results in a worse optimum, being
on or above the line needs some more explanation.

Then, notice that while most optimizers with low cavity residue lie on the line. Some deviate below the line, and some go slightly
above. Again, deviating below the line makes sense, as cavity equation has multiple solutions, and not all of them may be optimal,
but the deviation above, even though slightly, needs more explanation.

I think that deviations above the line can have two reasons:
\begin{enumerate}
    \item $\bar{h}^a$ and $\bar{h}^v$ recovered are not optimal.
    \item The constraints are exploited to obtain higher values.
\end{enumerate}

The first reason does not lead to the significant upwards deviation. To see this, we simply need to project $\bar{h}^a$ and $\bar{h}^v$
to a nearby solution of the cavity equation. This does not significantly move the optimal line upwards, in some cases, it even moved it
downwards.
\begin{figure}[!h]
    \centering
    \includegraphics[width=18cm]{img/Ising_bptt/OBJ_l=3_k=3_solve_h.png}
    \caption{Objective function values for numerical, now compared with optima that are projected to cavity equation solutions.}
    \label{Fig.ISBP-OBJ-l3k3-solveh}
\end{figure}

\newpage

We can then test the second theory. It seems unlikely that constraint is being exploited, given that constraints are
being satisfied to $10^{-10}$ accuracy. However, the following example shows that even small deviations in constraint
may lead to big deviations in function value. To investigate, we select the high cavity residue point for $B=0.1$,
$\beta=-0.87$, or the top left deviation point in the $B=0.1$ plot.

The distribution looks like the following:
\begin{figure}[h]
    \centering
    \includegraphics[width=15cm]{img/Ising_bptt/DIST_sample_large_cavity_res.png}
    \caption{One of the numerical solutions to $B=0.1$, $\beta=-0.87$, $l=3$, $k=3$.}
    \label{Fig.ISBP-DIST-SMPL}
\end{figure}

We can see that the distributions almost normalized, with a residue of about $10^{-12}$. If we plug this into the
distribution directly, it has a value of about $1.56$, as shown by the figure above. However, if we reduce all the
small values in this distribution to zero, leaving a normalized, symmetric distribution, we now only have
objective function value of about $1.21$. A small deviation in the constraint pushes the objective way beyond
the supposed optimal value.

I did not check each distribution individually, but I still think that this is the main reason why small deviations
from constraint pushed the objective value way higher than it should be.

\newpage

\subsection{Cross-check Optimizer}

Excluding the results with high cavity equation residue, we should see that the rest are close to the optimal distribution
as described in (\ref{Eq.ISBP-soln}). Indeed, higher deviation from analytical optima happens only when higher cavity equation
residues are present:
\begin{figure}[h]
    \centering
    \includegraphics[width=15cm]{img/Ising_bptt/SOLN_l=3_k=3.png}
    \caption{Analytical solution deviation from numerical results, measured by relative entropy. $h$ recovered and from
        numerical solution and projected to cavity equation solution. Low deviation scheme is dominated by solutions
        with low cavity residues.}
    \label{Fig.ISBP-SOLN-compare-l3k3}
\end{figure}

\end{document}